{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6859e72f",
   "metadata": {},
   "source": [
    "# GraphRAG Toolkit: Simple GraphRAG Workflow\n",
    "\n",
    "This notebook demonstrates a minimal end-to-end GraphRAG workflow using the [AWS GraphRAG Toolkit](https://github.com/awslabs/graphrag-toolkit). It covers:\n",
    "\n",
    "1. Installing the GraphRAG Toolkit and dependencies\n",
    "2. Configuring connections to a graph store and vector store\n",
    "3. Indexing a small corpus into a **hierarchical lexical graph**\n",
    "4. Querying the graph using the `LexicalGraphQueryEngine`\n",
    "\n",
    "The example uses AWS Neptune (graph store) and Amazon OpenSearch Serverless (vector store), which is the reference setup used in the official AWS blog and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0449b4",
   "metadata": {},
   "source": [
    "## 1. Prerequisites and Architecture\n",
    "\n",
    "To run this notebook end-to-end you will need:\n",
    "\n",
    "- An AWS account\n",
    "- An **Amazon Neptune Database or Neptune Analytics** graph instance\n",
    "- An **Amazon OpenSearch Serverless** collection for vector storage\n",
    "- Access to **Amazon Bedrock** foundation models (for extraction and embeddings)\n",
    "- Appropriate IAM permissions for Neptune, OpenSearch Serverless, and Bedrock\n",
    "\n",
    "Set the following environment variables in your Jupyter environment (for example via a `.env` file or the notebook UI):\n",
    "\n",
    "- `GRAPH_STORE` – Neptune endpoint, e.g. `neptune-db://my-graph.cluster-abcdefghijkl.us-east-1.neptune.amazonaws.com`\n",
    "- `VECTOR_STORE` – OpenSearch Serverless endpoint, e.g. `aoss://https://abcdefghijkl.us-east-1.aoss.amazonaws.com`\n",
    "- `GRAPHRAG_EXTRACTION_LLM` (optional) – Bedrock model ID used for extraction, e.g. `anthropic.claude-3-haiku-20240307-v1:0`\n",
    "\n",
    "If these variables are not set, the notebook will fall back to placeholder values that you must replace before running the indexing and querying cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fbf3c6",
   "metadata": {},
   "source": [
    "## 2. Install GraphRAG Toolkit and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61716055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest GraphRAG Toolkit build from GitHub\n",
    "# (This pulls a published ZIP artifact with all toolkit components.)\n",
    "%pip install \"https://github.com/awslabs/graphrag-toolkit/releases/latest/download/graphrag-toolkit.zip\"\n",
    "\n",
    "# Helpful utilities\n",
    "%pip install \"nest_asyncio\" \"python-dotenv\" \"llama-index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc3a72",
   "metadata": {},
   "source": [
    "## 3. Imports and Runtime Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69decc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from graphrag_toolkit import LexicalGraphIndex, LexicalGraphQueryEngine, GraphRAGConfig\n",
    "from graphrag_toolkit.storage import GraphStoreFactory, VectorStoreFactory\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# Allow nested event loops (needed in some notebook environments)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables from a local .env file if present\n",
    "load_dotenv()\n",
    "\n",
    "# Read configuration from environment, falling back to obvious placeholders\n",
    "GRAPH_STORE = os.environ.get(\n",
    "    \"GRAPH_STORE\",\n",
    "    \"neptune-db://<your-neptune-endpoint>\",\n",
    ")\n",
    "VECTOR_STORE = os.environ.get(\n",
    "    \"VECTOR_STORE\",\n",
    "    \"aoss://https://<your-opensearch-endpoint>\",\n",
    ")\n",
    "\n",
    "# Configure the LLM used during the extraction phase\n",
    "# (Override via GRAPHRAG_EXTRACTION_LLM if you want a different Bedrock model.)\n",
    "GraphRAGConfig.extraction_llm = os.environ.get(\n",
    "    \"GRAPHRAG_EXTRACTION_LLM\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    ")\n",
    "\n",
    "print(\"GRAPH_STORE:\", GRAPH_STORE)\n",
    "print(\"VECTOR_STORE:\", VECTOR_STORE)\n",
    "print(\"Extraction LLM:\", GraphRAGConfig.extraction_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb86ba5",
   "metadata": {},
   "source": [
    "## 4. Indexing: Build a Hierarchical Lexical Graph\n",
    "\n",
    "In this step we:\n",
    "\n",
    "1. Connect to the configured graph and vector stores\n",
    "2. Load a small corpus (several Amazon Neptune documentation pages)\n",
    "3. Use `LexicalGraphIndex.extract_and_build(...)` to:\n",
    "   - Chunk the documents\n",
    "   - Run LLM-based extraction to create statements, entities, topics, and facts\n",
    "   - Persist the resulting **hierarchical lexical graph** and embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fde5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the graph and vector stores\n",
    "graph_store = GraphStoreFactory.for_graph_store(GRAPH_STORE)\n",
    "vector_store = VectorStoreFactory.for_vector_store(VECTOR_STORE)\n",
    "\n",
    "# Create the lexical graph index\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    ")\n",
    "\n",
    "# Example corpus: a few public Neptune docs pages\n",
    "doc_urls = [\n",
    "    \"https://docs.aws.amazon.com/neptune/latest/userguide/intro.html\",\n",
    "    \"https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html\",\n",
    "    \"https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html\",\n",
    "    \"https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html\",\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url: {\"url\": url},\n",
    ").load_data(doc_urls)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents. Starting extract_and_build() ...\")\n",
    "\n",
    "# This single call runs both the Extract and Build phases and may take several minutes\n",
    "# depending on corpus size and LLM configuration.\n",
    "graph_index.extract_and_build(docs, show_progress=True)\n",
    "\n",
    "print(\"\\nIndexing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39f44b",
   "metadata": {},
   "source": [
    "## 5. Querying: Graph-Enhanced Retrieval with `LexicalGraphQueryEngine`\n",
    "\n",
    "Now that the lexical graph is populated, we can query it using the `LexicalGraphQueryEngine`.\n",
    "\n",
    "The example below:\n",
    "\n",
    "1. Reconnects to the same graph and vector stores\n",
    "2. Creates a query engine configured for **traversal-based search**\n",
    "3. Asks a multi-document question about Neptune Database vs Neptune Analytics\n",
    "4. Prints the generated answer (and, if available, some supporting source URLs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect (in real deployments, indexing and querying might run in separate processes)\n",
    "graph_store = GraphStoreFactory.for_graph_store(GRAPH_STORE)\n",
    "vector_store = VectorStoreFactory.for_vector_store(VECTOR_STORE)\n",
    "\n",
    "# Create a traversal-based query engine\n",
    "query_engine = LexicalGraphQueryEngine.for_traversal_based_search(\n",
    "    graph_store,\n",
    "    vector_store,\n",
    ")\n",
    "\n",
    "question = \"What are the differences between Neptune Database and Neptune Analytics?\"\n",
    "response = query_engine.query(question)\n",
    "\n",
    "print(\"Question:\\n\", question)\n",
    "print(\"\\nAnswer:\\n\", response.response)\n",
    "\n",
    "# If the response object exposes supporting nodes, print a few for inspection\n",
    "source_nodes = getattr(response, \"source_nodes\", None)\n",
    "if source_nodes:\n",
    "    print(\"\\nSupporting sources (first few):\")\n",
    "    for node in source_nodes[:5]:\n",
    "        url = None\n",
    "        # Different response implementations attach metadata differently; be defensive.\n",
    "        if hasattr(node, \"metadata\") and isinstance(node.metadata, dict):\n",
    "            url = node.metadata.get(\"url\")\n",
    "        elif hasattr(node, \"node\") and hasattr(node.node, \"metadata\"):\n",
    "            url = node.node.metadata.get(\"url\")\n",
    "        if url:\n",
    "            print(\"-\", url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c704f1",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "This notebook showed a minimal GraphRAG workflow:\n",
    "\n",
    "1. **Indexing** content into a lexical graph with `LexicalGraphIndex`\n",
    "2. **Querying** via `LexicalGraphQueryEngine` using traversal-based search\n",
    "\n",
    "From here you can experiment with more advanced capabilities of the GraphRAG Toolkit:\n",
    "\n",
    "- Separating **Extract** and **Build** phases (checkpoints, re‑building from extracted data)\n",
    "- Customizing extraction pipelines (e.g., simplifying sentences, entity/relationship extraction controls)\n",
    "- Trying different retrievers such as the `SemanticGuidedRetriever` for hybrid semantic + graph search\n",
    "- Swapping in your own corpus (S3 documents, JSON data, or internal documentation)\n",
    "\n",
    "Refer to the GraphRAG Toolkit documentation and example notebooks in the repository for more advanced patterns."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
