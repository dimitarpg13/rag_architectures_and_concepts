{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üï∏Ô∏è AWS GraphRAG Toolkit Demo\n",
        "\n",
        "This notebook demonstrates how to use **AWS's GraphRAG Toolkit** for building graph-enhanced Retrieval-Augmented Generation (RAG) applications. The toolkit integrates with Amazon Neptune (graph database), Amazon OpenSearch Serverless (vector store), and Amazon Bedrock (foundation models).\n",
        "\n",
        "## What is GraphRAG?\n",
        "\n",
        "GraphRAG is an advanced RAG approach that:\n",
        "- **Extracts entities and relationships** from documents to build a knowledge graph\n",
        "- **Creates hierarchical lexical graphs** for structured understanding\n",
        "- **Combines graph traversal with vector search** for comprehensive retrieval\n",
        "- **Leverages Amazon Bedrock** for LLM-powered entity extraction and response generation\n",
        "\n",
        "## AWS Services Used\n",
        "\n",
        "| Service | Purpose |\n",
        "|---------|--------|\n",
        "| **Amazon Neptune** | Graph database for storing entities and relationships |\n",
        "| **Amazon OpenSearch Serverless** | Vector store for semantic search |\n",
        "| **Amazon Bedrock** | Foundation models (Claude, Titan) for extraction & generation |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Installation\n",
        "\n",
        "First, let's install the GraphRAG Toolkit and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install AWS GraphRAG Toolkit from GitHub\n",
        "%pip install git+https://github.com/awslabs/graphrag-toolkit.git --quiet\n",
        "\n",
        "# Install additional dependencies\n",
        "%pip install boto3 python-dotenv pyyaml networkx matplotlib pandas --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß 2. AWS Environment Setup\n",
        "\n",
        "The GraphRAG Toolkit requires AWS credentials and access to the following services:\n",
        "- **Amazon Neptune** (Database or Analytics)\n",
        "- **Amazon OpenSearch Serverless**\n",
        "- **Amazon Bedrock**\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "1. Set up an Amazon Neptune cluster or Neptune Analytics graph\n",
        "2. Create an Amazon OpenSearch Serverless collection\n",
        "3. Enable Amazon Bedrock models in your region (Claude Sonnet, Titan Embeddings)\n",
        "4. Configure IAM roles with appropriate permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file if it exists\n",
        "load_dotenv()\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_REGION = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
        "\n",
        "# Neptune Configuration\n",
        "NEPTUNE_ENDPOINT = os.environ.get(\"NEPTUNE_ENDPOINT\", \"<your-neptune-endpoint>\")\n",
        "NEPTUNE_PORT = os.environ.get(\"NEPTUNE_PORT\", \"8182\")\n",
        "\n",
        "# OpenSearch Serverless Configuration\n",
        "OPENSEARCH_ENDPOINT = os.environ.get(\"OPENSEARCH_ENDPOINT\", \"<your-opensearch-endpoint>\")\n",
        "\n",
        "# Bedrock Model Configuration\n",
        "LLM_MODEL_ID = os.environ.get(\"LLM_MODEL_ID\", \"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
        "EMBEDDING_MODEL_ID = os.environ.get(\"EMBEDDING_MODEL_ID\", \"amazon.titan-embed-text-v2:0\")\n",
        "\n",
        "print(f\"üåç AWS Region: {AWS_REGION}\")\n",
        "print(f\"üîó Neptune Endpoint: {NEPTUNE_ENDPOINT}\")\n",
        "print(f\"üîç OpenSearch Endpoint: {OPENSEARCH_ENDPOINT}\")\n",
        "print(f\"ü§ñ LLM Model: {LLM_MODEL_ID}\")\n",
        "print(f\"üìä Embedding Model: {EMBEDDING_MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify AWS credentials\n",
        "def verify_aws_credentials():\n",
        "    \"\"\"Verify AWS credentials are configured correctly.\"\"\"\n",
        "    try:\n",
        "        sts = boto3.client('sts', region_name=AWS_REGION)\n",
        "        identity = sts.get_caller_identity()\n",
        "        print(f\"‚úÖ AWS credentials configured\")\n",
        "        print(f\"   Account: {identity['Account']}\")\n",
        "        print(f\"   User/Role: {identity['Arn']}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå AWS credentials error: {e}\")\n",
        "        print(\"   Please configure AWS credentials using:\")\n",
        "        print(\"   - AWS CLI: aws configure\")\n",
        "        print(\"   - Environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\")\n",
        "        print(\"   - IAM role (for EC2/Lambda)\")\n",
        "        return False\n",
        "\n",
        "verify_aws_credentials()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ 3. Project Structure Setup\n",
        "\n",
        "Let's create the project directory structure for our GraphRAG application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define project directories\n",
        "PROJECT_DIR = Path.cwd()\n",
        "INPUT_DIR = PROJECT_DIR / \"input\"\n",
        "OUTPUT_DIR = PROJECT_DIR / \"output\"\n",
        "\n",
        "# Create directories\n",
        "INPUT_DIR.mkdir(exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Project directory: {PROJECT_DIR}\")\n",
        "print(f\"üìÇ Input directory: {INPUT_DIR}\")\n",
        "print(f\"üìÇ Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ 4. Sample Data Preparation\n",
        "\n",
        "For this demo, we'll create a sample text document about a fictional tech company (same as the Microsoft GraphRAG demo for comparison)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text about a fictional tech company for demonstration\n",
        "sample_text = \"\"\"\n",
        "# TechCorp Innovation Report 2025\n",
        "\n",
        "## Company Overview\n",
        "\n",
        "TechCorp is a leading technology company founded in 2015 by Sarah Chen and Michael Rodriguez in San Francisco.\n",
        "The company specializes in artificial intelligence solutions for enterprise customers. With over 5,000 employees\n",
        "across 20 offices worldwide, TechCorp has become a major player in the AI industry.\n",
        "\n",
        "## Leadership Team\n",
        "\n",
        "Sarah Chen serves as the CEO and has led the company through multiple successful funding rounds. She previously\n",
        "worked at Google and Stanford AI Lab. Michael Rodriguez, the CTO, oversees all technical operations and R&D.\n",
        "He holds a PhD in Machine Learning from MIT.\n",
        "\n",
        "The CFO, Jennifer Park, joined in 2019 from Goldman Sachs. She has been instrumental in the company's financial\n",
        "growth and successful IPO in 2023. David Thompson leads the Sales division and has expanded the customer base\n",
        "to include Fortune 500 companies like Amazon, Microsoft, and Walmart.\n",
        "\n",
        "## Products and Services\n",
        "\n",
        "TechCorp's flagship product, \"AIAssist Pro\", is an enterprise AI assistant that helps companies automate\n",
        "customer service operations. It uses advanced natural language processing and has been deployed by over\n",
        "200 enterprise customers.\n",
        "\n",
        "\"DataSense Analytics\" is the company's second major product, offering predictive analytics for supply chain\n",
        "optimization. Major clients include Walmart and Target, who have reported 30% efficiency improvements.\n",
        "\n",
        "The newest product, \"SecureAI\", launched in 2024, focuses on AI-powered cybersecurity. It has already\n",
        "attracted partnerships with three major banks: JPMorgan Chase, Bank of America, and Wells Fargo.\n",
        "\n",
        "## Research and Development\n",
        "\n",
        "TechCorp's R&D division, led by Dr. Emily Watson, has published over 50 papers in top AI conferences.\n",
        "The team recently made a breakthrough in efficient transformer architectures, reducing compute costs by 40%.\n",
        "\n",
        "The company collaborates with Stanford University, MIT, and Carnegie Mellon on various research projects.\n",
        "Dr. Watson's team includes researchers from DeepMind, OpenAI, and Google Brain.\n",
        "\n",
        "## Financial Performance\n",
        "\n",
        "In 2024, TechCorp reported revenue of $2.5 billion, a 45% increase from the previous year. The company's\n",
        "market cap reached $50 billion after the successful IPO. Major investors include Sequoia Capital,\n",
        "Andreessen Horowitz, and SoftBank Vision Fund.\n",
        "\n",
        "## Future Plans\n",
        "\n",
        "TechCorp plans to expand into the healthcare AI market in 2025, with partnerships already in place with\n",
        "Mayo Clinic and Cleveland Clinic. The company is also developing autonomous systems for logistics,\n",
        "working with FedEx and UPS on pilot programs.\n",
        "\n",
        "Sarah Chen announced plans to open new R&D centers in London, Singapore, and Tel Aviv to attract\n",
        "global talent and serve international customers better.\n",
        "\"\"\"\n",
        "\n",
        "# Save the sample text to the input directory\n",
        "input_file = INPUT_DIR / \"techcorp_report.txt\"\n",
        "input_file.write_text(sample_text)\n",
        "\n",
        "print(f\"‚úÖ Sample document saved to: {input_file}\")\n",
        "print(f\"üìù Document length: {len(sample_text)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 5. Configuration\n",
        "\n",
        "Let's create a configuration for the AWS GraphRAG Toolkit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# GraphRAG Toolkit configuration\n",
        "config = {\n",
        "    \"graph_store\": {\n",
        "        \"type\": \"neptune\",\n",
        "        \"endpoint\": NEPTUNE_ENDPOINT,\n",
        "        \"port\": int(NEPTUNE_PORT),\n",
        "        \"region\": AWS_REGION,\n",
        "        \"iam_auth\": True\n",
        "    },\n",
        "    \"vector_store\": {\n",
        "        \"type\": \"opensearch_serverless\",\n",
        "        \"endpoint\": OPENSEARCH_ENDPOINT,\n",
        "        \"region\": AWS_REGION,\n",
        "        \"index_name\": \"graphrag-demo-index\"\n",
        "    },\n",
        "    \"llm\": {\n",
        "        \"provider\": \"bedrock\",\n",
        "        \"model_id\": LLM_MODEL_ID,\n",
        "        \"region\": AWS_REGION,\n",
        "        \"max_tokens\": 4096,\n",
        "        \"temperature\": 0\n",
        "    },\n",
        "    \"embedding\": {\n",
        "        \"provider\": \"bedrock\",\n",
        "        \"model_id\": EMBEDDING_MODEL_ID,\n",
        "        \"region\": AWS_REGION\n",
        "    },\n",
        "    \"extraction\": {\n",
        "        \"chunk_size\": 1200,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"entity_types\": [\"PERSON\", \"ORGANIZATION\", \"PRODUCT\", \"LOCATION\", \"EVENT\"],\n",
        "        \"max_entities_per_chunk\": 20\n",
        "    },\n",
        "    \"input\": {\n",
        "        \"directory\": str(INPUT_DIR),\n",
        "        \"file_pattern\": \"*.txt\"\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"directory\": str(OUTPUT_DIR)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save configuration to YAML file\n",
        "config_file = PROJECT_DIR / \"config.yaml\"\n",
        "with open(config_file, 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "print(f\"‚úÖ Configuration saved to: {config_file}\")\n",
        "print(\"\\nüìã Configuration preview:\")\n",
        "print(yaml.dump(config, default_flow_style=False, sort_keys=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç 6. Initialize GraphRAG Toolkit\n",
        "\n",
        "Let's verify the GraphRAG Toolkit is installed correctly and initialize the components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we can import graphrag_toolkit\n",
        "try:\n",
        "    import graphrag_toolkit\n",
        "    print(f\"‚úÖ GraphRAG Toolkit imported successfully\")\n",
        "    \n",
        "    # List available modules\n",
        "    modules = [attr for attr in dir(graphrag_toolkit) if not attr.startswith('_')]\n",
        "    print(f\"\\nüì¶ Available modules: {modules[:10]}...\" if len(modules) > 10 else f\"\\nüì¶ Available modules: {modules}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå GraphRAG Toolkit not installed: {e}\")\n",
        "    print(\"\\nRun: pip install git+https://github.com/awslabs/graphrag-toolkit.git\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è 7. Entity Extraction with Amazon Bedrock\n",
        "\n",
        "The GraphRAG Toolkit uses Amazon Bedrock to extract entities and relationships from documents.\n",
        "\n",
        "‚ö†Ô∏è **Note**: This step requires:\n",
        "- Active AWS credentials with Bedrock access\n",
        "- Running Neptune database\n",
        "- Running OpenSearch Serverless collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entity extraction using Amazon Bedrock\n",
        "import json\n",
        "\n",
        "def extract_entities_with_bedrock(text: str, model_id: str = LLM_MODEL_ID) -> dict:\n",
        "    \"\"\"\n",
        "    Extract entities and relationships from text using Amazon Bedrock.\n",
        "    \n",
        "    This demonstrates the core extraction logic used by the GraphRAG Toolkit.\n",
        "    \"\"\"\n",
        "    bedrock = boto3.client('bedrock-runtime', region_name=AWS_REGION)\n",
        "    \n",
        "    extraction_prompt = f\"\"\"Analyze the following text and extract:\n",
        "1. ENTITIES: People, organizations, products, locations, and events mentioned\n",
        "2. RELATIONSHIPS: How these entities are connected to each other\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "Return your response in this JSON format:\n",
        "{{\n",
        "    \"entities\": [\n",
        "        {{\"name\": \"Entity Name\", \"type\": \"PERSON|ORGANIZATION|PRODUCT|LOCATION|EVENT\", \"description\": \"Brief description\"}}\n",
        "    ],\n",
        "    \"relationships\": [\n",
        "        {{\"source\": \"Entity1\", \"target\": \"Entity2\", \"relationship\": \"Description of relationship\"}}\n",
        "    ]\n",
        "}}\n",
        "\n",
        "Only return valid JSON, no additional text.\"\"\"\n",
        "    \n",
        "    # Call Bedrock with Claude\n",
        "    body = json.dumps({\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 4096,\n",
        "        \"temperature\": 0,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": extraction_prompt}\n",
        "        ]\n",
        "    })\n",
        "    \n",
        "    try:\n",
        "        response = bedrock.invoke_model(\n",
        "            modelId=model_id,\n",
        "            body=body,\n",
        "            contentType='application/json',\n",
        "            accept='application/json'\n",
        "        )\n",
        "        \n",
        "        response_body = json.loads(response['body'].read())\n",
        "        result_text = response_body['content'][0]['text']\n",
        "        \n",
        "        # Parse JSON response\n",
        "        return json.loads(result_text)\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Bedrock error: {e}\")\n",
        "        return {\"entities\": [], \"relationships\": []}\n",
        "\n",
        "# Extract entities from sample text\n",
        "print(\"üöÄ Extracting entities and relationships using Amazon Bedrock...\")\n",
        "print(\"‚è≥ This may take a moment...\\n\")\n",
        "\n",
        "try:\n",
        "    extraction_result = extract_entities_with_bedrock(sample_text)\n",
        "    \n",
        "    print(f\"‚úÖ Extraction complete!\")\n",
        "    print(f\"   Entities found: {len(extraction_result.get('entities', []))}\")\n",
        "    print(f\"   Relationships found: {len(extraction_result.get('relationships', []))}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during extraction: {e}\")\n",
        "    print(\"\\nüí° Make sure you have:\")\n",
        "    print(\"   - Valid AWS credentials\")\n",
        "    print(\"   - Access to Amazon Bedrock in your region\")\n",
        "    print(f\"   - Enabled the {LLM_MODEL_ID} model\")\n",
        "    extraction_result = {\"entities\": [], \"relationships\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display extracted entities\n",
        "import pandas as pd\n",
        "\n",
        "if extraction_result.get('entities'):\n",
        "    entities_df = pd.DataFrame(extraction_result['entities'])\n",
        "    print(\"üè∑Ô∏è Extracted Entities:\")\n",
        "    display(entities_df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No entities extracted. Check Bedrock configuration.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display extracted relationships\n",
        "if extraction_result.get('relationships'):\n",
        "    rels_df = pd.DataFrame(extraction_result['relationships'])\n",
        "    print(\"üîó Extracted Relationships:\")\n",
        "    display(rels_df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No relationships extracted. Check Bedrock configuration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 8. Store in Amazon Neptune\n",
        "\n",
        "The extracted entities and relationships are stored in Amazon Neptune as a knowledge graph.\n",
        "\n",
        "‚ö†Ô∏è **Note**: This requires an active Neptune cluster or Neptune Analytics graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_in_neptune(entities: list, relationships: list, endpoint: str = NEPTUNE_ENDPOINT):\n",
        "    \"\"\"\n",
        "    Store extracted entities and relationships in Amazon Neptune.\n",
        "    \n",
        "    This uses the openCypher query language for Neptune.\n",
        "    \"\"\"\n",
        "    from botocore.auth import SigV4Auth\n",
        "    from botocore.awsrequest import AWSRequest\n",
        "    import requests\n",
        "    \n",
        "    neptune_url = f\"https://{endpoint}:{NEPTUNE_PORT}/openCypher\"\n",
        "    \n",
        "    session = boto3.Session()\n",
        "    credentials = session.get_credentials()\n",
        "    \n",
        "    def execute_query(query: str):\n",
        "        \"\"\"Execute an openCypher query on Neptune with IAM auth.\"\"\"\n",
        "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
        "        data = f\"query={query}\"\n",
        "        \n",
        "        request = AWSRequest(method='POST', url=neptune_url, data=data, headers=headers)\n",
        "        SigV4Auth(credentials, 'neptune-db', AWS_REGION).add_auth(request)\n",
        "        \n",
        "        response = requests.post(\n",
        "            neptune_url,\n",
        "            data=data,\n",
        "            headers=dict(request.headers)\n",
        "        )\n",
        "        return response\n",
        "    \n",
        "    # Create entities\n",
        "    print(\"üì§ Creating entity nodes...\")\n",
        "    for entity in entities:\n",
        "        name = entity['name'].replace(\"'\", \"\\\\'\")\n",
        "        entity_type = entity.get('type', 'ENTITY')\n",
        "        description = entity.get('description', '').replace(\"'\", \"\\\\'\")\n",
        "        \n",
        "        query = f\"\"\"\n",
        "        MERGE (e:{entity_type} {{name: '{name}'}})\n",
        "        SET e.description = '{description}'\n",
        "        RETURN e\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            execute_query(query)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Failed to create {name}: {e}\")\n",
        "    \n",
        "    # Create relationships\n",
        "    print(\"üì§ Creating relationships...\")\n",
        "    for rel in relationships:\n",
        "        source = rel['source'].replace(\"'\", \"\\\\'\")\n",
        "        target = rel['target'].replace(\"'\", \"\\\\'\")\n",
        "        relationship = rel.get('relationship', 'RELATED_TO').replace(\"'\", \"\\\\'\")\n",
        "        \n",
        "        query = f\"\"\"\n",
        "        MATCH (s {{name: '{source}'}}), (t {{name: '{target}'}})\n",
        "        MERGE (s)-[r:RELATED_TO {{description: '{relationship}'}}]->(t)\n",
        "        RETURN r\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            execute_query(query)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Failed to create relationship: {e}\")\n",
        "    \n",
        "    print(f\"‚úÖ Stored {len(entities)} entities and {len(relationships)} relationships\")\n",
        "\n",
        "# Store in Neptune (uncomment when Neptune is configured)\n",
        "# store_in_neptune(extraction_result.get('entities', []), extraction_result.get('relationships', []))\n",
        "\n",
        "print(\"‚ö†Ô∏è Neptune storage is commented out. Uncomment after configuring Neptune endpoint.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîé 9. Querying the Knowledge Graph\n",
        "\n",
        "The GraphRAG Toolkit supports various query strategies that combine:\n",
        "- **Graph traversal** for entity-centric queries\n",
        "- **Vector search** for semantic similarity\n",
        "- **LLM generation** for natural language responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_graphrag(question: str, entities: list, relationships: list, model_id: str = LLM_MODEL_ID) -> str:\n",
        "    \"\"\"\n",
        "    Answer questions using the extracted knowledge graph.\n",
        "    \n",
        "    This demonstrates the query strategy used by the GraphRAG Toolkit.\n",
        "    \"\"\"\n",
        "    bedrock = boto3.client('bedrock-runtime', region_name=AWS_REGION)\n",
        "    \n",
        "    # Build context from knowledge graph\n",
        "    context = \"Knowledge Graph Information:\\n\\n\"\n",
        "    \n",
        "    context += \"ENTITIES:\\n\"\n",
        "    for entity in entities:\n",
        "        context += f\"- {entity['name']} ({entity.get('type', 'UNKNOWN')}): {entity.get('description', '')}\\n\"\n",
        "    \n",
        "    context += \"\\nRELATIONSHIPS:\\n\"\n",
        "    for rel in relationships:\n",
        "        context += f\"- {rel['source']} -> {rel['target']}: {rel.get('relationship', '')}\\n\"\n",
        "    \n",
        "    query_prompt = f\"\"\"You are a helpful assistant that answers questions based on a knowledge graph.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer the question based only on the information provided in the knowledge graph. \n",
        "If the information is not available, say so.\n",
        "Be concise but comprehensive.\"\"\"\n",
        "    \n",
        "    body = json.dumps({\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 2048,\n",
        "        \"temperature\": 0,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": query_prompt}\n",
        "        ]\n",
        "    })\n",
        "    \n",
        "    try:\n",
        "        response = bedrock.invoke_model(\n",
        "            modelId=model_id,\n",
        "            body=body,\n",
        "            contentType='application/json',\n",
        "            accept='application/json'\n",
        "        )\n",
        "        \n",
        "        response_body = json.loads(response['body'].read())\n",
        "        return response_body['content'][0]['text']\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error querying: {e}\"\n",
        "\n",
        "def run_query(question: str):\n",
        "    \"\"\"Run a GraphRAG query and display results.\"\"\"\n",
        "    print(f\"üîç Query: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if extraction_result.get('entities'):\n",
        "        response = query_graphrag(\n",
        "            question,\n",
        "            extraction_result.get('entities', []),\n",
        "            extraction_result.get('relationships', [])\n",
        "        )\n",
        "        print(f\"\\nüìù Response:\\n{response}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No knowledge graph available. Run entity extraction first.\")\n",
        "    \n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Queries\n",
        "\n",
        "Let's test our GraphRAG system with some questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entity-specific query\n",
        "run_query(\"Who is Sarah Chen and what is her role at TechCorp?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Product-focused query\n",
        "run_query(\"What products does TechCorp offer?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relationship query\n",
        "run_query(\"What universities does TechCorp collaborate with?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthesis query\n",
        "run_query(\"Summarize TechCorp's business strategy and future plans.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® 10. Visualizing the Knowledge Graph\n",
        "\n",
        "Let's create a visualization of the extracted knowledge graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_knowledge_graph(entities: list, relationships: list):\n",
        "    \"\"\"Create a visualization of the knowledge graph.\"\"\"\n",
        "    if not entities:\n",
        "        print(\"‚ö†Ô∏è No entities to visualize. Run extraction first.\")\n",
        "        return\n",
        "    \n",
        "    # Create graph\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add nodes\n",
        "    for entity in entities:\n",
        "        G.add_node(entity['name'], type=entity.get('type', 'UNKNOWN'))\n",
        "    \n",
        "    # Add edges\n",
        "    for rel in relationships:\n",
        "        if rel['source'] in G.nodes and rel['target'] in G.nodes:\n",
        "            G.add_edge(rel['source'], rel['target'])\n",
        "    \n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    \n",
        "    # Color nodes by type\n",
        "    node_types = nx.get_node_attributes(G, 'type')\n",
        "    unique_types = list(set(node_types.values()))\n",
        "    \n",
        "    # Define colors for entity types\n",
        "    type_colors = {\n",
        "        'PERSON': '#FF6B6B',\n",
        "        'ORGANIZATION': '#4ECDC4',\n",
        "        'PRODUCT': '#95E1D3',\n",
        "        'LOCATION': '#F38181',\n",
        "        'EVENT': '#AA96DA',\n",
        "        'UNKNOWN': '#CCCCCC'\n",
        "    }\n",
        "    \n",
        "    colors = [type_colors.get(node_types.get(node, 'UNKNOWN'), '#CCCCCC') for node in G.nodes()]\n",
        "    \n",
        "    # Layout\n",
        "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
        "    \n",
        "    # Draw\n",
        "    nx.draw(G, pos,\n",
        "            node_color=colors,\n",
        "            node_size=1500,\n",
        "            font_size=8,\n",
        "            font_weight='bold',\n",
        "            with_labels=True,\n",
        "            edge_color='#888888',\n",
        "            alpha=0.9,\n",
        "            width=1.5)\n",
        "    \n",
        "    plt.title(\"AWS GraphRAG Knowledge Graph\", fontsize=16, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Add legend\n",
        "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                   markerfacecolor=type_colors.get(t, '#CCCCCC'),\n",
        "                                   markersize=12, label=t)\n",
        "                       for t in unique_types]\n",
        "    plt.legend(handles=legend_elements, loc='upper left', title='Entity Types', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PROJECT_DIR / 'knowledge_graph.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nüìä Graph Statistics:\")\n",
        "    print(f\"   Nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"   Edges: {G.number_of_edges()}\")\n",
        "    print(f\"   Entity Types: {unique_types}\")\n",
        "\n",
        "# Visualize the knowledge graph\n",
        "visualize_knowledge_graph(\n",
        "    extraction_result.get('entities', []),\n",
        "    extraction_result.get('relationships', [])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà 11. Vector Embeddings with Amazon Titan\n",
        "\n",
        "For semantic search, the GraphRAG Toolkit generates vector embeddings using Amazon Titan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_embeddings(texts: list, model_id: str = EMBEDDING_MODEL_ID) -> list:\n",
        "    \"\"\"\n",
        "    Generate vector embeddings using Amazon Titan.\n",
        "    \"\"\"\n",
        "    bedrock = boto3.client('bedrock-runtime', region_name=AWS_REGION)\n",
        "    embeddings = []\n",
        "    \n",
        "    for text in texts:\n",
        "        body = json.dumps({\n",
        "            \"inputText\": text\n",
        "        })\n",
        "        \n",
        "        try:\n",
        "            response = bedrock.invoke_model(\n",
        "                modelId=model_id,\n",
        "                body=body,\n",
        "                contentType='application/json',\n",
        "                accept='application/json'\n",
        "            )\n",
        "            \n",
        "            response_body = json.loads(response['body'].read())\n",
        "            embeddings.append(response_body['embedding'])\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Embedding error for '{text[:30]}...': {e}\")\n",
        "            embeddings.append(None)\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings for entity descriptions\n",
        "if extraction_result.get('entities'):\n",
        "    print(\"üî¢ Generating embeddings for entities...\")\n",
        "    \n",
        "    entity_texts = [f\"{e['name']}: {e.get('description', '')}\" for e in extraction_result['entities']]\n",
        "    \n",
        "    try:\n",
        "        entity_embeddings = generate_embeddings(entity_texts[:5])  # Limit for demo\n",
        "        \n",
        "        valid_embeddings = [e for e in entity_embeddings if e is not None]\n",
        "        if valid_embeddings:\n",
        "            print(f\"‚úÖ Generated {len(valid_embeddings)} embeddings\")\n",
        "            print(f\"   Embedding dimension: {len(valid_embeddings[0])}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No embeddings generated. Check Bedrock configuration.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating embeddings: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No entities available. Run extraction first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ 12. Cleanup\n",
        "\n",
        "Optional: Remove generated files and clean up resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"Remove all generated files.\"\"\"\n",
        "    dirs_to_remove = ['input', 'output']\n",
        "    files_to_remove = ['config.yaml', 'knowledge_graph.png']\n",
        "    \n",
        "    for dir_name in dirs_to_remove:\n",
        "        dir_path = PROJECT_DIR / dir_name\n",
        "        if dir_path.exists():\n",
        "            shutil.rmtree(dir_path)\n",
        "            print(f\"üóëÔ∏è Removed directory: {dir_path}\")\n",
        "    \n",
        "    for file_name in files_to_remove:\n",
        "        file_path = PROJECT_DIR / file_name\n",
        "        if file_path.exists():\n",
        "            file_path.unlink()\n",
        "            print(f\"üóëÔ∏è Removed file: {file_path}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Cleanup complete!\")\n",
        "\n",
        "# Uncomment to run cleanup\n",
        "# cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, we covered:\n",
        "\n",
        "1. **Installation** of AWS GraphRAG Toolkit\n",
        "2. **AWS Configuration** for Neptune, OpenSearch, and Bedrock\n",
        "3. **Project Setup** with proper directory structure\n",
        "4. **Entity Extraction** using Amazon Bedrock (Claude)\n",
        "5. **Knowledge Graph Storage** in Amazon Neptune\n",
        "6. **Querying** the knowledge graph with natural language\n",
        "7. **Visualization** of extracted entities and relationships\n",
        "8. **Vector Embeddings** with Amazon Titan\n",
        "\n",
        "## AWS Services Required\n",
        "\n",
        "| Service | Purpose | Cost Tier |\n",
        "|---------|---------|----------|\n",
        "| Amazon Neptune | Graph database | Pay per instance/hour |\n",
        "| Amazon OpenSearch Serverless | Vector store | Pay per OCU-hour |\n",
        "| Amazon Bedrock | LLM & Embeddings | Pay per token |\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [AWS GraphRAG Toolkit GitHub](https://github.com/awslabs/graphrag-toolkit)\n",
        "- [Amazon Neptune Documentation](https://docs.aws.amazon.com/neptune/)\n",
        "- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
        "- [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html)\n",
        "- [GraphRAG Toolkit Blog Post](https://aws.amazon.com/blogs/database/introducing-the-graphrag-toolkit/)\n",
        "\n",
        "## Comparison with Microsoft GraphRAG\n",
        "\n",
        "| Feature | AWS GraphRAG Toolkit | Microsoft GraphRAG |\n",
        "|---------|---------------------|-------------------|\n",
        "| Graph Store | Amazon Neptune | File-based (Parquet) |\n",
        "| Vector Store | OpenSearch Serverless | Built-in |\n",
        "| LLM Provider | Amazon Bedrock | OpenAI |\n",
        "| Embeddings | Amazon Titan | OpenAI |\n",
        "| Deployment | AWS Infrastructure | Local/Cloud |\n",
        "| Community Detection | Neptune Analytics | Leiden Algorithm |\n",
        "\n",
        "## Tips\n",
        "\n",
        "- Use **Neptune Analytics** for serverless graph queries\n",
        "- Enable **Bedrock model access** in your AWS region before use\n",
        "- Consider **cost optimization** by using smaller models for development\n",
        "- Use **IAM roles** for secure access to AWS services"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
