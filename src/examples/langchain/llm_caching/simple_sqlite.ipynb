{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4cbf2458",
      "metadata": {
        "id": "4cbf2458"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/rag_architectures_and_concepts/blob/main/src/examples/langchain//llm_caching/simple_sqlite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b843b5c4",
      "metadata": {
        "id": "b843b5c4"
      },
      "source": [
        "# How to cache LLM responses\n",
        "\n",
        "LangChain provides an optional [caching](/docs/concepts/chat_models/#caching) layer for LLMs. This is useful for two reasons:\n",
        "\n",
        "It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\n",
        "It can speed up your application by reducing the number of API calls you make to the LLM provider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "25b0b0fa",
      "metadata": {
        "id": "25b0b0fa",
        "outputId": "155b07e7-3b24-4e2e-8fda-130b67908ef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain_openai langchain_community dotenv\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
        "\n",
        "\n",
        "# Please manually enter OpenAI Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0aa6d335",
      "metadata": {
        "id": "0aa6d335"
      },
      "outputs": [],
      "source": [
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# To make the caching really obvious, let's use a slower and older model.\n",
        "# Caching supports newer chat models as well.\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f168ff0d",
      "metadata": {
        "id": "f168ff0d",
        "outputId": "07f46578-8af2-4164-a7f0-d42de8018194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 50 ms, sys: 12.8 ms, total: 62.7 ms\n",
            "Wall time: 1.5 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "%%time\n",
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ce7620fb",
      "metadata": {
        "id": "ce7620fb",
        "outputId": "18f4bb67-d813-4eee-db64-9e6bd4117933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 325 µs, sys: 70 µs, total: 395 µs\n",
            "Wall time: 400 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ab452f4",
      "metadata": {
        "id": "4ab452f4"
      },
      "source": [
        "## SQLite Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2e65de83",
      "metadata": {
        "id": "2e65de83",
        "outputId": "d4562ae5-84df-4d5f-cb6d-b4593de3281b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '.langchain.db': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm .langchain.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0be83715",
      "metadata": {
        "id": "0be83715"
      },
      "outputs": [],
      "source": [
        "# We can do the same thing with a SQLite cache\n",
        "from langchain_community.cache import SQLiteCache\n",
        "\n",
        "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9b427ce7",
      "metadata": {
        "id": "9b427ce7",
        "outputId": "8da2d34a-a6ec-425d-cfba-fceb2c1ea9e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 33.1 ms, sys: 2.05 ms, total: 35.2 ms\n",
            "Wall time: 1.15 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "%%time\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "87f52611",
      "metadata": {
        "id": "87f52611",
        "outputId": "26615b52-9f82-45fb-e49c-8c78b198b11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.62 ms, sys: 110 µs, total: 2.73 ms\n",
            "Wall time: 2.42 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9bb158",
      "metadata": {
        "id": "6a9bb158"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
