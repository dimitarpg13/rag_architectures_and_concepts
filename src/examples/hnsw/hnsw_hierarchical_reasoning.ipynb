{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç Hierarchical Reasoning via Hierarchical Navigable Small World (HNSW)\n",
        "\n",
        "This notebook demonstrates **Hierarchical Navigable Small World (HNSW)** graphs and how they enable efficient hierarchical reasoning for similarity search and retrieval-augmented generation (RAG) applications.\n",
        "\n",
        "## What is HNSW?\n",
        "\n",
        "HNSW is a graph-based algorithm for **Approximate Nearest Neighbor (ANN)** search that achieves:\n",
        "- **Logarithmic search complexity**: O(log N) vs O(N) for brute force\n",
        "- **High recall**: Often 95%+ accuracy compared to exact search\n",
        "- **Hierarchical structure**: Multiple layers for efficient navigation\n",
        "\n",
        "## Hierarchical Structure\n",
        "\n",
        "```\n",
        "Layer 2 (sparse):     [A] -------- [B]\n",
        "                       |            |\n",
        "Layer 1 (medium):     [A] -- [C] -- [B] -- [D]\n",
        "                       |     |      |      |\n",
        "Layer 0 (dense):      [A]-[E]-[C]-[F]-[B]-[G]-[D]-[H]\n",
        "```\n",
        "\n",
        "- **Higher layers**: Fewer nodes, longer connections (coarse navigation)\n",
        "- **Lower layers**: More nodes, shorter connections (fine navigation)\n",
        "- **Search**: Start at top layer, navigate down through layers\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Installation\n",
        "\n",
        "Install the required libraries for HNSW and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install hnswlib numpy matplotlib scikit-learn --quiet\n",
        "%pip install sentence-transformers --quiet\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import hnswlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 2. Understanding HNSW Basics\n",
        "\n",
        "Let's start with a simple example to understand how HNSW works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data: 1000 points in 128 dimensions\n",
        "np.random.seed(42)\n",
        "num_elements = 1000\n",
        "dim = 128\n",
        "\n",
        "# Create clustered data for more interesting hierarchical structure\n",
        "data, labels = make_blobs(n_samples=num_elements, n_features=dim, centers=10, random_state=42)\n",
        "data = data.astype('float32')\n",
        "\n",
        "# Normalize vectors (for cosine similarity)\n",
        "data = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
        "\n",
        "print(f\"üìä Data shape: {data.shape}\")\n",
        "print(f\"üìä Number of clusters: {len(np.unique(labels))}\")\n",
        "print(f\"üìä Data range: [{data.min():.3f}, {data.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create HNSW index\n",
        "# Key parameters:\n",
        "# - M: Number of bi-directional links per element (affects memory and search quality)\n",
        "# - ef_construction: Size of dynamic candidate list during construction\n",
        "\n",
        "# Initialize the index\n",
        "hnsw_index = hnswlib.Index(space='cosine', dim=dim)  # 'cosine', 'l2', or 'ip'\n",
        "\n",
        "# Initialize index - max_elements is the maximum number of elements\n",
        "hnsw_index.init_index(\n",
        "    max_elements=num_elements,\n",
        "    ef_construction=200,  # Higher = better quality, slower construction\n",
        "    M=16                   # Number of connections per layer\n",
        ")\n",
        "\n",
        "# Add data to the index\n",
        "start_time = time.time()\n",
        "hnsw_index.add_items(data, np.arange(num_elements))\n",
        "build_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚úÖ HNSW index built in {build_time:.3f} seconds\")\n",
        "print(f\"üìä Index parameters:\")\n",
        "print(f\"   - M (connections): {hnsw_index.M}\")\n",
        "print(f\"   - ef_construction: {hnsw_index.ef_construction}\")\n",
        "print(f\"   - Max elements: {hnsw_index.max_elements}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîé 3. Search: Hierarchical Navigation\n",
        "\n",
        "The search process navigates through layers:\n",
        "1. **Start at top layer** with a random entry point\n",
        "2. **Greedy search** to find closest node in current layer\n",
        "3. **Move down** to next layer using the found node as entry point\n",
        "4. **Repeat** until reaching layer 0\n",
        "5. **Return** the k nearest neighbors from layer 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for nearest neighbors\n",
        "# ef (search parameter): Size of dynamic candidate list during search\n",
        "# Higher ef = better recall but slower search\n",
        "\n",
        "# Set search parameter\n",
        "hnsw_index.set_ef(50)  # ef should be >= k (number of neighbors to return)\n",
        "\n",
        "# Query vector (use first data point as query)\n",
        "query = data[0:1]  # Shape: (1, 128)\n",
        "k = 10  # Find 10 nearest neighbors\n",
        "\n",
        "# Search\n",
        "start_time = time.time()\n",
        "labels_result, distances = hnsw_index.knn_query(query, k=k)\n",
        "search_time = time.time() - start_time\n",
        "\n",
        "print(f\"üîç Query completed in {search_time*1000:.3f} ms\")\n",
        "print(f\"\\nüìä Top {k} nearest neighbors:\")\n",
        "print(f\"{'Rank':<6} {'ID':<8} {'Distance':<12} {'Similarity':<12}\")\n",
        "print(\"-\" * 40)\n",
        "for i, (idx, dist) in enumerate(zip(labels_result[0], distances[0])):\n",
        "    # For cosine space, distance = 1 - similarity\n",
        "    similarity = 1 - dist\n",
        "    print(f\"{i+1:<6} {idx:<8} {dist:<12.6f} {similarity:<12.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with brute-force search\n",
        "def brute_force_search(data: np.ndarray, query: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Exact nearest neighbor search using brute force.\"\"\"\n",
        "    similarities = cosine_similarity(query, data)[0]\n",
        "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
        "    top_k_similarities = similarities[top_k_indices]\n",
        "    return top_k_indices, 1 - top_k_similarities  # Convert to distances\n",
        "\n",
        "# Brute force search\n",
        "start_time = time.time()\n",
        "bf_indices, bf_distances = brute_force_search(data, query, k)\n",
        "bf_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Search Time Comparison:\")\n",
        "print(f\"   HNSW:        {search_time*1000:.3f} ms\")\n",
        "print(f\"   Brute Force: {bf_time*1000:.3f} ms\")\n",
        "print(f\"   Speedup:     {bf_time/search_time:.1f}x faster\")\n",
        "\n",
        "# Calculate recall (how many of the true nearest neighbors did HNSW find?)\n",
        "hnsw_set = set(labels_result[0])\n",
        "bf_set = set(bf_indices)\n",
        "recall = len(hnsw_set.intersection(bf_set)) / k * 100\n",
        "\n",
        "print(f\"\\nüìä Recall: {recall:.1f}% ({int(recall*k/100)}/{k} correct neighbors)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è 4. Visualizing the Hierarchical Structure\n",
        "\n",
        "Let's visualize how HNSW creates a hierarchical structure for efficient navigation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce dimensions for visualization\n",
        "pca = PCA(n_components=2)\n",
        "data_2d = pca.fit_transform(data)\n",
        "\n",
        "# Visualize the data with cluster colors\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Data points colored by cluster\n",
        "plt.subplot(1, 2, 1)\n",
        "scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='tab10', alpha=0.6, s=20)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.title('Data Distribution (10 Clusters)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "\n",
        "# Plot 2: Highlight query and its neighbors\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(data_2d[:, 0], data_2d[:, 1], c='lightgray', alpha=0.3, s=20, label='All points')\n",
        "plt.scatter(data_2d[labels_result[0], 0], data_2d[labels_result[0], 1], \n",
        "            c='blue', s=100, label='HNSW neighbors', edgecolors='black')\n",
        "plt.scatter(data_2d[0, 0], data_2d[0, 1], c='red', s=200, marker='*', label='Query', edgecolors='black')\n",
        "plt.title('Query and Retrieved Neighbors')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† 5. Hierarchical Reasoning with HNSW\n",
        "\n",
        "Now let's implement **hierarchical reasoning** - using HNSW for multi-level semantic search and retrieval.\n",
        "\n",
        "### Structure:\n",
        "- **Level 0 (Documents)**: Full document embeddings (coarse)\n",
        "- **Level 1 (Sections)**: Section/paragraph embeddings (medium)\n",
        "- **Level 2 (Sentences)**: Sentence-level embeddings (fine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HierarchicalKnowledgeBase:\n",
        "    \"\"\"\n",
        "    A hierarchical knowledge base using HNSW for multi-level reasoning.\n",
        "    \n",
        "    This enables hierarchical reasoning:\n",
        "    1. Find relevant documents (coarse)\n",
        "    2. Find relevant sections within those documents (medium)\n",
        "    3. Find specific sentences (fine)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dim: int = 384):\n",
        "        self.dim = dim\n",
        "        self.levels = {}\n",
        "        self.metadata = {}\n",
        "        \n",
        "    def add_level(self, level_name: str, embeddings: np.ndarray, \n",
        "                  metadata: List[Dict], M: int = 16, ef_construction: int = 200):\n",
        "        \"\"\"Add a hierarchical level to the knowledge base.\"\"\"\n",
        "        n_items = len(embeddings)\n",
        "        \n",
        "        # Create HNSW index for this level\n",
        "        index = hnswlib.Index(space='cosine', dim=self.dim)\n",
        "        index.init_index(max_elements=n_items, ef_construction=ef_construction, M=M)\n",
        "        index.add_items(embeddings.astype('float32'), np.arange(n_items))\n",
        "        \n",
        "        self.levels[level_name] = {\n",
        "            'index': index,\n",
        "            'embeddings': embeddings,\n",
        "            'count': n_items\n",
        "        }\n",
        "        self.metadata[level_name] = metadata\n",
        "        \n",
        "        print(f\"‚úÖ Added level '{level_name}' with {n_items} items\")\n",
        "    \n",
        "    def search_level(self, level_name: str, query_embedding: np.ndarray, \n",
        "                     k: int = 5, ef: int = 50) -> List[Dict]:\n",
        "        \"\"\"Search within a specific level.\"\"\"\n",
        "        if level_name not in self.levels:\n",
        "            raise ValueError(f\"Level '{level_name}' not found\")\n",
        "        \n",
        "        index = self.levels[level_name]['index']\n",
        "        index.set_ef(ef)\n",
        "        \n",
        "        labels, distances = index.knn_query(query_embedding.reshape(1, -1).astype('float32'), k=k)\n",
        "        \n",
        "        results = []\n",
        "        for idx, dist in zip(labels[0], distances[0]):\n",
        "            results.append({\n",
        "                'id': int(idx),\n",
        "                'distance': float(dist),\n",
        "                'similarity': float(1 - dist),\n",
        "                'metadata': self.metadata[level_name][idx]\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def hierarchical_search(self, query_embedding: np.ndarray,\n",
        "                           levels: List[str] = None,\n",
        "                           k_per_level: List[int] = None) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"Perform hierarchical search across multiple levels.\"\"\"\n",
        "        if levels is None:\n",
        "            levels = list(self.levels.keys())\n",
        "        if k_per_level is None:\n",
        "            k_per_level = [5] * len(levels)\n",
        "        \n",
        "        results = {}\n",
        "        for level_name, k in zip(levels, k_per_level):\n",
        "            level_results = self.search_level(level_name, query_embedding, k=k)\n",
        "            results[level_name] = level_results\n",
        "        \n",
        "        return results\n",
        "\n",
        "print(\"‚úÖ HierarchicalKnowledgeBase class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö 6. Example: Hierarchical Document Search\n",
        "\n",
        "Let's create a practical example with simulated document hierarchies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate hierarchical document structure\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "num_documents = 50\n",
        "sections_per_doc = 5\n",
        "sentences_per_section = 10\n",
        "embedding_dim = 128\n",
        "\n",
        "# Create document topics\n",
        "topics = [\"Machine Learning\", \"Natural Language Processing\", \"Computer Vision\", \n",
        "          \"Reinforcement Learning\", \"Neural Networks\"]\n",
        "\n",
        "# Generate embeddings with hierarchical structure\n",
        "# Documents within same topic will have similar embeddings\n",
        "\n",
        "# Level 0: Documents\n",
        "doc_embeddings = []\n",
        "doc_metadata = []\n",
        "\n",
        "for doc_id in range(num_documents):\n",
        "    topic_id = doc_id % len(topics)\n",
        "    # Base embedding for topic\n",
        "    topic_base = np.random.randn(embedding_dim) * 0.5\n",
        "    topic_base[topic_id * 25:(topic_id + 1) * 25] += 2\n",
        "    # Add document-specific variation\n",
        "    doc_emb = topic_base + np.random.randn(embedding_dim) * 0.2\n",
        "    doc_emb = doc_emb / np.linalg.norm(doc_emb)\n",
        "    \n",
        "    doc_embeddings.append(doc_emb)\n",
        "    doc_metadata.append({\n",
        "        'doc_id': doc_id,\n",
        "        'title': f\"Document {doc_id}: {topics[topic_id]} Guide Part {doc_id // len(topics) + 1}\",\n",
        "        'topic': topics[topic_id]\n",
        "    })\n",
        "\n",
        "doc_embeddings = np.array(doc_embeddings, dtype='float32')\n",
        "\n",
        "# Level 1: Sections\n",
        "section_embeddings = []\n",
        "section_metadata = []\n",
        "\n",
        "for doc_id in range(num_documents):\n",
        "    for sec_id in range(sections_per_doc):\n",
        "        sec_emb = doc_embeddings[doc_id] + np.random.randn(embedding_dim) * 0.15\n",
        "        sec_emb = sec_emb / np.linalg.norm(sec_emb)\n",
        "        \n",
        "        section_embeddings.append(sec_emb)\n",
        "        section_metadata.append({\n",
        "            'section_id': len(section_embeddings) - 1,\n",
        "            'parent_doc_id': doc_id,\n",
        "            'title': f\"Section {sec_id + 1} of Doc {doc_id}\",\n",
        "            'topic': doc_metadata[doc_id]['topic']\n",
        "        })\n",
        "\n",
        "section_embeddings = np.array(section_embeddings, dtype='float32')\n",
        "\n",
        "# Level 2: Sentences\n",
        "sentence_embeddings = []\n",
        "sentence_metadata = []\n",
        "\n",
        "for sec_idx, sec_emb in enumerate(section_embeddings):\n",
        "    for sent_id in range(sentences_per_section):\n",
        "        sent_emb = sec_emb + np.random.randn(embedding_dim) * 0.1\n",
        "        sent_emb = sent_emb / np.linalg.norm(sent_emb)\n",
        "        \n",
        "        sentence_embeddings.append(sent_emb)\n",
        "        sentence_metadata.append({\n",
        "            'sentence_id': len(sentence_embeddings) - 1,\n",
        "            'parent_section_id': sec_idx,\n",
        "            'parent_doc_id': section_metadata[sec_idx]['parent_doc_id'],\n",
        "            'text': f\"Sentence {sent_id + 1} about {section_metadata[sec_idx]['topic']}\",\n",
        "            'topic': section_metadata[sec_idx]['topic']\n",
        "        })\n",
        "\n",
        "sentence_embeddings = np.array(sentence_embeddings, dtype='float32')\n",
        "\n",
        "print(f\"üìö Created hierarchical document structure:\")\n",
        "print(f\"   Level 0 (Documents):  {len(doc_embeddings):,} items\")\n",
        "print(f\"   Level 1 (Sections):   {len(section_embeddings):,} items\")\n",
        "print(f\"   Level 2 (Sentences):  {len(sentence_embeddings):,} items\")\n",
        "print(f\"   Total:                {len(doc_embeddings) + len(section_embeddings) + len(sentence_embeddings):,} items\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create hierarchical knowledge base\n",
        "kb = HierarchicalKnowledgeBase(dim=embedding_dim)\n",
        "\n",
        "# Add levels\n",
        "kb.add_level('documents', doc_embeddings, doc_metadata, M=16, ef_construction=100)\n",
        "kb.add_level('sections', section_embeddings, section_metadata, M=16, ef_construction=100)\n",
        "kb.add_level('sentences', sentence_embeddings, sentence_metadata, M=16, ef_construction=100)\n",
        "\n",
        "print(\"\\n‚úÖ Hierarchical knowledge base ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a query embedding (simulate a query about \"Neural Networks\")\n",
        "query_topic_id = 4  # Neural Networks\n",
        "query_embedding = np.random.randn(embedding_dim) * 0.3\n",
        "query_embedding[query_topic_id * 25:(query_topic_id + 1) * 25] += 2\n",
        "query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
        "query_embedding = query_embedding.astype('float32')\n",
        "\n",
        "print(\"üîç Query: Find information about Neural Networks\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Perform hierarchical search\n",
        "results = kb.hierarchical_search(\n",
        "    query_embedding,\n",
        "    levels=['documents', 'sections', 'sentences'],\n",
        "    k_per_level=[3, 5, 10]\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"\\nüìÑ LEVEL 0: Top Documents\")\n",
        "print(\"-\" * 60)\n",
        "for r in results['documents']:\n",
        "    print(f\"  [{r['similarity']:.3f}] {r['metadata']['title']}\")\n",
        "    print(f\"           Topic: {r['metadata']['topic']}\")\n",
        "\n",
        "print(\"\\nüìë LEVEL 1: Top Sections\")\n",
        "print(\"-\" * 60)\n",
        "for r in results['sections'][:5]:\n",
        "    print(f\"  [{r['similarity']:.3f}] {r['metadata']['title']}\")\n",
        "    print(f\"           Parent Doc: {r['metadata']['parent_doc_id']}, Topic: {r['metadata']['topic']}\")\n",
        "\n",
        "print(\"\\nüìù LEVEL 2: Top Sentences\")\n",
        "print(\"-\" * 60)\n",
        "for r in results['sentences'][:5]:\n",
        "    print(f\"  [{r['similarity']:.3f}] {r['metadata']['text']}\")\n",
        "    print(f\"           Section: {r['metadata']['parent_section_id']}, Doc: {r['metadata']['parent_doc_id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì 7. Summary & Best Practices\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **HNSW Structure**:\n",
        "   - Multiple layers with decreasing density\n",
        "   - Higher layers for coarse navigation\n",
        "   - Lower layers for fine-grained search\n",
        "\n",
        "2. **Hierarchical Reasoning**:\n",
        "   - Build indices at multiple semantic levels\n",
        "   - Use appropriate strategy based on query type\n",
        "   - Combine coarse-to-fine or fine-to-coarse approaches\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "| Parameter | Recommendation | Trade-off |\n",
        "|-----------|---------------|----------|\n",
        "| **M** | 16-64 | Higher = better quality, more memory |\n",
        "| **ef_construction** | 100-500 | Higher = better index, slower build |\n",
        "| **ef** (search) | 50-200 | Higher = better recall, slower search |\n",
        "\n",
        "### Reasoning Strategies\n",
        "\n",
        "| Strategy | Use Case |\n",
        "|----------|----------|\n",
        "| **Top-Down** | Exploratory queries, need context |\n",
        "| **Bottom-Up** | Precise fact retrieval |\n",
        "| **Multi-Hop** | Complex reasoning, multiple evidence |\n",
        "\n",
        "---\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [HNSW Paper](https://arxiv.org/abs/1603.09320)\n",
        "- [hnswlib Documentation](https://github.com/nmslib/hnswlib)\n",
        "- [FAISS (Facebook AI Similarity Search)](https://github.com/facebookresearch/faiss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚úÖ Notebook complete!\")\n",
        "print(\"\\nüìö Key takeaways:\")\n",
        "print(\"   1. HNSW provides O(log N) search complexity\")\n",
        "print(\"   2. Hierarchical structure enables multi-level reasoning\")\n",
        "print(\"   3. Different strategies suit different query types\")\n",
        "print(\"   4. Trade-offs between speed, memory, and accuracy are configurable\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
