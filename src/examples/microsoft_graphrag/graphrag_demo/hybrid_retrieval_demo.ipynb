{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”€ Hybrid Retrieval Demo: Vector DB + Knowledge Graph\n",
        "\n",
        "This notebook demonstrates **hybrid retrieval** combining:\n",
        "- **Vector Database (ChromaDB)**: Semantic similarity search using embeddings\n",
        "- **In-Memory Knowledge Graph (NetworkX)**: Graph-based traversal and relationship reasoning\n",
        "\n",
        "## Why Hybrid Retrieval?\n",
        "\n",
        "| Approach | Strengths | Weaknesses |\n",
        "|----------|-----------|------------|\n",
        "| **Vector Search** | Semantic similarity, handles paraphrasing | Misses structural relationships |\n",
        "| **Graph Search** | Captures relationships, multi-hop reasoning | Limited semantic understanding |\n",
        "| **Hybrid** | Best of both worlds | More complex implementation |\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "```\n",
        "Query â†’ [Vector Search] â†’ Semantic Matches\n",
        "     â†˜                   â†—\n",
        "       [Fusion Strategy] â†’ Final Results\n",
        "     â†—                   â†˜\n",
        "Query â†’ [Graph Search]  â†’ Relational Matches\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ 1. Installation\n",
        "\n",
        "Install the required libraries for vector database, graph processing, and embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install chromadb sentence-transformers networkx matplotlib --quiet\n",
        "%pip install numpy pandas scikit-learn --quiet\n",
        "%pip install openai tiktoken --quiet\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional, Any, Set\n",
        "from dataclasses import dataclass, field\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ 2. Configuration & Sample Data\n",
        "\n",
        "We'll use sample data about a fictional tech company (same as the original GraphRAG demo) to demonstrate hybrid retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents about TechCorp (enriched version)\n",
        "DOCUMENTS = [\n",
        "    {\n",
        "        \"id\": \"doc_1\",\n",
        "        \"title\": \"Company Overview\",\n",
        "        \"content\": \"\"\"TechCorp is a leading technology company founded in 2015 by Sarah Chen and Michael Rodriguez \n",
        "        in San Francisco. The company specializes in artificial intelligence solutions for enterprise customers. \n",
        "        With over 5,000 employees across 20 offices worldwide, TechCorp has become a major player in the AI industry.\n",
        "        Sarah Chen serves as the CEO and has led the company through multiple successful funding rounds.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_2\", \n",
        "        \"title\": \"Leadership Team\",\n",
        "        \"content\": \"\"\"Sarah Chen serves as the CEO and has led the company through multiple successful funding rounds.\n",
        "        She previously worked at Google and Stanford AI Lab. Michael Rodriguez, the CTO, oversees all technical \n",
        "        operations and R&D. He holds a PhD in Machine Learning from MIT. The CFO, Jennifer Park, joined in 2019 \n",
        "        from Goldman Sachs. She has been instrumental in the company's financial growth and successful IPO in 2023.\n",
        "        David Thompson leads the Sales division and has expanded the customer base to include Fortune 500 companies.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_3\",\n",
        "        \"title\": \"Products and Services\", \n",
        "        \"content\": \"\"\"TechCorp's flagship product, AIAssist Pro, is an enterprise AI assistant that helps companies \n",
        "        automate customer service operations. It uses advanced natural language processing and has been deployed \n",
        "        by over 200 enterprise customers. DataSense Analytics is the company's second major product, offering \n",
        "        predictive analytics for supply chain optimization. Major clients include Walmart and Target, who have \n",
        "        reported 30% efficiency improvements. The newest product, SecureAI, launched in 2024, focuses on \n",
        "        AI-powered cybersecurity. It has attracted partnerships with JPMorgan Chase, Bank of America, and Wells Fargo.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_4\",\n",
        "        \"title\": \"Research and Development\",\n",
        "        \"content\": \"\"\"TechCorp's R&D division, led by Dr. Emily Watson, has published over 50 papers in top AI \n",
        "        conferences. The team recently made a breakthrough in efficient transformer architectures, reducing \n",
        "        compute costs by 40%. The company collaborates with Stanford University, MIT, and Carnegie Mellon on \n",
        "        various research projects. Dr. Watson's team includes researchers from DeepMind, OpenAI, and Google Brain.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_5\",\n",
        "        \"title\": \"Financial Performance\",\n",
        "        \"content\": \"\"\"In 2024, TechCorp reported revenue of $2.5 billion, a 45% increase from the previous year.\n",
        "        The company's market cap reached $50 billion after the successful IPO. Major investors include \n",
        "        Sequoia Capital, Andreessen Horowitz, and SoftBank Vision Fund. The IPO was led by Jennifer Park \n",
        "        and raised significant capital for expansion.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_6\",\n",
        "        \"title\": \"Future Plans\",\n",
        "        \"content\": \"\"\"TechCorp plans to expand into the healthcare AI market in 2025, with partnerships already \n",
        "        in place with Mayo Clinic and Cleveland Clinic. The company is also developing autonomous systems for \n",
        "        logistics, working with FedEx and UPS on pilot programs. Sarah Chen announced plans to open new R&D \n",
        "        centers in London, Singapore, and Tel Aviv to attract global talent and serve international customers.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_7\",\n",
        "        \"title\": \"Technology Stack\",\n",
        "        \"content\": \"\"\"TechCorp's AI platform is built on a proprietary transformer architecture developed by \n",
        "        Michael Rodriguez and his team. The platform runs on cloud infrastructure from AWS and Google Cloud.\n",
        "        AIAssist Pro uses a fine-tuned large language model with over 100 billion parameters. The system \n",
        "        processes over 10 million customer queries daily across all deployed instances.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_8\",\n",
        "        \"title\": \"Partnerships and Clients\",\n",
        "        \"content\": \"\"\"TechCorp has strategic partnerships with major tech companies including Amazon, Microsoft, \n",
        "        and Google. The company serves over 500 enterprise clients across various industries. Key clients \n",
        "        include Walmart, Target, JPMorgan Chase, Bank of America, Wells Fargo, FedEx, and UPS. The sales \n",
        "        team led by David Thompson has grown revenue from enterprise clients by 60% year-over-year.\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“š Loaded {len(DOCUMENTS)} documents\")\n",
        "for doc in DOCUMENTS:\n",
        "    print(f\"   - {doc['title']}: {len(doc['content'])} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ—ï¸ 3. Core Data Structures\n",
        "\n",
        "Define the data classes for entities, relationships, and retrieval results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Entity:\n",
        "    \"\"\"Represents an extracted entity from the knowledge base.\"\"\"\n",
        "    id: str\n",
        "    name: str\n",
        "    type: str  # PERSON, ORGANIZATION, PRODUCT, LOCATION, etc.\n",
        "    description: str\n",
        "    source_docs: List[str] = field(default_factory=list)\n",
        "    attributes: Dict[str, Any] = field(default_factory=dict)\n",
        "    \n",
        "    def to_text(self) -> str:\n",
        "        \"\"\"Convert entity to searchable text.\"\"\"\n",
        "        return f\"{self.name} ({self.type}): {self.description}\"\n",
        "\n",
        "\n",
        "@dataclass \n",
        "class Relationship:\n",
        "    \"\"\"Represents a relationship between two entities.\"\"\"\n",
        "    id: str\n",
        "    source: str  # Entity ID\n",
        "    target: str  # Entity ID\n",
        "    relation_type: str  # WORKS_AT, FOUNDED, DEVELOPS, etc.\n",
        "    description: str\n",
        "    weight: float = 1.0\n",
        "    source_docs: List[str] = field(default_factory=list)\n",
        "    \n",
        "    def to_text(self) -> str:\n",
        "        \"\"\"Convert relationship to searchable text.\"\"\"\n",
        "        return f\"{self.source} {self.relation_type} {self.target}: {self.description}\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RetrievalResult:\n",
        "    \"\"\"Represents a single retrieval result from any source.\"\"\"\n",
        "    id: str\n",
        "    content: str\n",
        "    score: float\n",
        "    source: str  # 'vector', 'graph', or 'hybrid'\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"RetrievalResult(id={self.id}, score={self.score:.3f}, source={self.source})\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HybridResult:\n",
        "    \"\"\"Aggregated result from hybrid retrieval.\"\"\"\n",
        "    query: str\n",
        "    vector_results: List[RetrievalResult]\n",
        "    graph_results: List[RetrievalResult]\n",
        "    fused_results: List[RetrievalResult]\n",
        "    fusion_method: str\n",
        "    \n",
        "    def summary(self) -> str:\n",
        "        return (f\"Query: {self.query}\\n\"\n",
        "                f\"Vector results: {len(self.vector_results)}\\n\"\n",
        "                f\"Graph results: {len(self.graph_results)}\\n\"\n",
        "                f\"Fused results: {len(self.fused_results)}\\n\"\n",
        "                f\"Fusion method: {self.fusion_method}\")\n",
        "\n",
        "\n",
        "print(\"âœ… Data classes defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ—„ï¸ 4. Vector Store (ChromaDB)\n",
        "\n",
        "The Vector Store handles semantic similarity search using embeddings. We use ChromaDB as an in-memory vector database with sentence transformers for embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorStore:\n",
        "    \"\"\"\n",
        "    Vector database for semantic similarity search using ChromaDB.\n",
        "    \n",
        "    Supports multiple collections:\n",
        "    - documents: Full document chunks\n",
        "    - entities: Entity descriptions\n",
        "    - relationships: Relationship descriptions\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"Initialize the vector store with an embedding model.\"\"\"\n",
        "        self.embedding_model_name = embedding_model\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
        "        \n",
        "        # Initialize ChromaDB client (in-memory)\n",
        "        self.client = chromadb.Client()\n",
        "        self.collections: Dict[str, chromadb.Collection] = {}\n",
        "        \n",
        "        print(f\"âœ… VectorStore initialized with '{embedding_model}'\")\n",
        "        print(f\"   Embedding dimension: {self.embedding_dim}\")\n",
        "    \n",
        "    def create_collection(self, name: str) -> chromadb.Collection:\n",
        "        \"\"\"Create or get a collection.\"\"\"\n",
        "        if name in self.collections:\n",
        "            return self.collections[name]\n",
        "        \n",
        "        collection = self.client.get_or_create_collection(\n",
        "            name=name,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
        "        )\n",
        "        self.collections[name] = collection\n",
        "        return collection\n",
        "    \n",
        "    def add_documents(self, collection_name: str, documents: List[Dict[str, Any]]):\n",
        "        \"\"\"\n",
        "        Add documents to a collection.\n",
        "        \n",
        "        Args:\n",
        "            collection_name: Name of the collection\n",
        "            documents: List of dicts with 'id', 'content', and optional 'metadata'\n",
        "        \"\"\"\n",
        "        collection = self.create_collection(collection_name)\n",
        "        \n",
        "        ids = [doc['id'] for doc in documents]\n",
        "        texts = [doc['content'] for doc in documents]\n",
        "        metadatas = [doc.get('metadata', {}) for doc in documents]\n",
        "        \n",
        "        # Generate embeddings\n",
        "        embeddings = self.embedding_model.encode(texts, show_progress_bar=False).tolist()\n",
        "        \n",
        "        # Add to collection\n",
        "        collection.add(\n",
        "            ids=ids,\n",
        "            embeddings=embeddings,\n",
        "            documents=texts,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "        \n",
        "        print(f\"   Added {len(documents)} items to '{collection_name}'\")\n",
        "    \n",
        "    def add_entities(self, entities: List[Entity]):\n",
        "        \"\"\"Add entities to the entities collection.\"\"\"\n",
        "        docs = [{\n",
        "            'id': e.id,\n",
        "            'content': e.to_text(),\n",
        "            'metadata': {\n",
        "                'name': e.name,\n",
        "                'type': e.type,\n",
        "                'source_docs': ','.join(e.source_docs)\n",
        "            }\n",
        "        } for e in entities]\n",
        "        self.add_documents('entities', docs)\n",
        "    \n",
        "    def add_relationships(self, relationships: List[Relationship]):\n",
        "        \"\"\"Add relationships to the relationships collection.\"\"\"\n",
        "        docs = [{\n",
        "            'id': r.id,\n",
        "            'content': r.to_text(),\n",
        "            'metadata': {\n",
        "                'source': r.source,\n",
        "                'target': r.target,\n",
        "                'relation_type': r.relation_type\n",
        "            }\n",
        "        } for r in relationships]\n",
        "        self.add_documents('relationships', docs)\n",
        "    \n",
        "    def search(self, collection_name: str, query: str, k: int = 5) -> List[RetrievalResult]:\n",
        "        \"\"\"\n",
        "        Search a collection for similar documents.\n",
        "        \n",
        "        Args:\n",
        "            collection_name: Name of the collection to search\n",
        "            query: Query text\n",
        "            k: Number of results to return\n",
        "            \n",
        "        Returns:\n",
        "            List of RetrievalResult objects\n",
        "        \"\"\"\n",
        "        if collection_name not in self.collections:\n",
        "            raise ValueError(f\"Collection '{collection_name}' not found\")\n",
        "        \n",
        "        collection = self.collections[collection_name]\n",
        "        \n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.encode([query], show_progress_bar=False).tolist()\n",
        "        \n",
        "        # Search\n",
        "        results = collection.query(\n",
        "            query_embeddings=query_embedding,\n",
        "            n_results=min(k, collection.count()),\n",
        "            include=['documents', 'metadatas', 'distances']\n",
        "        )\n",
        "        \n",
        "        # Convert to RetrievalResult objects\n",
        "        retrieval_results = []\n",
        "        for i in range(len(results['ids'][0])):\n",
        "            # Convert distance to similarity score (ChromaDB returns L2 or cosine distance)\n",
        "            distance = results['distances'][0][i]\n",
        "            # For cosine distance in ChromaDB, similarity = 1 - distance\n",
        "            similarity = 1 - distance\n",
        "            \n",
        "            retrieval_results.append(RetrievalResult(\n",
        "                id=results['ids'][0][i],\n",
        "                content=results['documents'][0][i],\n",
        "                score=similarity,\n",
        "                source='vector',\n",
        "                metadata=results['metadatas'][0][i] if results['metadatas'] else {}\n",
        "            ))\n",
        "        \n",
        "        return retrieval_results\n",
        "    \n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Get the embedding for a text.\"\"\"\n",
        "        return self.embedding_model.encode([text], show_progress_bar=False)[0]\n",
        "\n",
        "\n",
        "print(\"âœ… VectorStore class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ•¸ï¸ 5. Knowledge Graph (NetworkX)\n",
        "\n",
        "The Knowledge Graph handles relationship-based retrieval using graph traversal algorithms. We use NetworkX for in-memory graph operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeGraph:\n",
        "    \"\"\"\n",
        "    In-memory knowledge graph using NetworkX for graph-based retrieval.\n",
        "    \n",
        "    Supports:\n",
        "    - Entity and relationship storage\n",
        "    - Multi-hop graph traversal\n",
        "    - Subgraph extraction\n",
        "    - Community detection\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize an empty knowledge graph.\"\"\"\n",
        "        self.graph = nx.DiGraph()  # Directed graph for relationships\n",
        "        self.entities: Dict[str, Entity] = {}\n",
        "        self.relationships: Dict[str, Relationship] = {}\n",
        "        self.entity_name_to_id: Dict[str, str] = {}\n",
        "        \n",
        "        print(\"âœ… KnowledgeGraph initialized\")\n",
        "    \n",
        "    def add_entity(self, entity: Entity):\n",
        "        \"\"\"Add an entity to the graph.\"\"\"\n",
        "        self.entities[entity.id] = entity\n",
        "        self.entity_name_to_id[entity.name.lower()] = entity.id\n",
        "        \n",
        "        # Add node with attributes\n",
        "        self.graph.add_node(\n",
        "            entity.id,\n",
        "            name=entity.name,\n",
        "            type=entity.type,\n",
        "            description=entity.description\n",
        "        )\n",
        "    \n",
        "    def add_relationship(self, relationship: Relationship):\n",
        "        \"\"\"Add a relationship to the graph.\"\"\"\n",
        "        self.relationships[relationship.id] = relationship\n",
        "        \n",
        "        # Add edge with attributes\n",
        "        self.graph.add_edge(\n",
        "            relationship.source,\n",
        "            relationship.target,\n",
        "            id=relationship.id,\n",
        "            relation_type=relationship.relation_type,\n",
        "            description=relationship.description,\n",
        "            weight=relationship.weight\n",
        "        )\n",
        "    \n",
        "    def get_entity_by_name(self, name: str) -> Optional[Entity]:\n",
        "        \"\"\"Get an entity by its name (case-insensitive).\"\"\"\n",
        "        entity_id = self.entity_name_to_id.get(name.lower())\n",
        "        return self.entities.get(entity_id) if entity_id else None\n",
        "    \n",
        "    def get_neighbors(self, entity_id: str, depth: int = 1, \n",
        "                      direction: str = 'both') -> Set[str]:\n",
        "        \"\"\"\n",
        "        Get neighboring entities up to a certain depth.\n",
        "        \n",
        "        Args:\n",
        "            entity_id: Starting entity ID\n",
        "            depth: Maximum traversal depth\n",
        "            direction: 'in', 'out', or 'both'\n",
        "        \"\"\"\n",
        "        if entity_id not in self.graph:\n",
        "            return set()\n",
        "        \n",
        "        neighbors = set()\n",
        "        current_level = {entity_id}\n",
        "        \n",
        "        for _ in range(depth):\n",
        "            next_level = set()\n",
        "            for node in current_level:\n",
        "                if direction in ['out', 'both']:\n",
        "                    next_level.update(self.graph.successors(node))\n",
        "                if direction in ['in', 'both']:\n",
        "                    next_level.update(self.graph.predecessors(node))\n",
        "            neighbors.update(next_level)\n",
        "            current_level = next_level - neighbors\n",
        "        \n",
        "        neighbors.discard(entity_id)  # Remove the starting entity\n",
        "        return neighbors\n",
        "    \n",
        "    def get_subgraph(self, entity_ids: List[str], include_neighbors: bool = True,\n",
        "                     neighbor_depth: int = 1) -> nx.DiGraph:\n",
        "        \"\"\"Extract a subgraph containing the specified entities.\"\"\"\n",
        "        nodes = set(entity_ids)\n",
        "        \n",
        "        if include_neighbors:\n",
        "            for entity_id in entity_ids:\n",
        "                nodes.update(self.get_neighbors(entity_id, depth=neighbor_depth))\n",
        "        \n",
        "        return self.graph.subgraph(nodes).copy()\n",
        "    \n",
        "    def find_paths(self, source_id: str, target_id: str, \n",
        "                   max_length: int = 3) -> List[List[str]]:\n",
        "        \"\"\"Find all paths between two entities up to a maximum length.\"\"\"\n",
        "        if source_id not in self.graph or target_id not in self.graph:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            paths = list(nx.all_simple_paths(\n",
        "                self.graph, source_id, target_id, cutoff=max_length\n",
        "            ))\n",
        "            return paths\n",
        "        except nx.NetworkXNoPath:\n",
        "            return []\n",
        "    \n",
        "    def search_by_entity(self, query_entities: List[str], \n",
        "                         depth: int = 2, k: int = 10) -> List[RetrievalResult]:\n",
        "        \"\"\"\n",
        "        Search the graph starting from query entities.\n",
        "        \n",
        "        Returns entities and relationships within the specified depth,\n",
        "        scored by relevance (closeness to query entities).\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        visited = set()\n",
        "        \n",
        "        # Find matching entities\n",
        "        matched_entity_ids = []\n",
        "        for query in query_entities:\n",
        "            entity = self.get_entity_by_name(query)\n",
        "            if entity:\n",
        "                matched_entity_ids.append(entity.id)\n",
        "        \n",
        "        if not matched_entity_ids:\n",
        "            return results\n",
        "        \n",
        "        # BFS to find related entities with distance-based scoring\n",
        "        for start_id in matched_entity_ids:\n",
        "            queue = [(start_id, 0)]  # (entity_id, distance)\n",
        "            \n",
        "            while queue and len(results) < k * 2:\n",
        "                current_id, dist = queue.pop(0)\n",
        "                \n",
        "                if current_id in visited or dist > depth:\n",
        "                    continue\n",
        "                visited.add(current_id)\n",
        "                \n",
        "                if current_id in self.entities:\n",
        "                    entity = self.entities[current_id]\n",
        "                    # Score decreases with distance\n",
        "                    score = 1.0 / (1.0 + dist)\n",
        "                    \n",
        "                    results.append(RetrievalResult(\n",
        "                        id=entity.id,\n",
        "                        content=entity.to_text(),\n",
        "                        score=score,\n",
        "                        source='graph',\n",
        "                        metadata={\n",
        "                            'name': entity.name,\n",
        "                            'type': entity.type,\n",
        "                            'distance': dist\n",
        "                        }\n",
        "                    ))\n",
        "                \n",
        "                # Add neighbors to queue\n",
        "                for neighbor in self.get_neighbors(current_id, depth=1):\n",
        "                    if neighbor not in visited:\n",
        "                        queue.append((neighbor, dist + 1))\n",
        "        \n",
        "        # Sort by score and return top k\n",
        "        results.sort(key=lambda x: x.score, reverse=True)\n",
        "        return results[:k]\n",
        "    \n",
        "    def get_relationship_context(self, entity_ids: List[str]) -> List[RetrievalResult]:\n",
        "        \"\"\"Get relationships involving the specified entities.\"\"\"\n",
        "        results = []\n",
        "        seen_rels = set()\n",
        "        \n",
        "        for entity_id in entity_ids:\n",
        "            # Outgoing relationships\n",
        "            for _, target, data in self.graph.out_edges(entity_id, data=True):\n",
        "                rel_id = data.get('id')\n",
        "                if rel_id and rel_id not in seen_rels:\n",
        "                    seen_rels.add(rel_id)\n",
        "                    rel = self.relationships.get(rel_id)\n",
        "                    if rel:\n",
        "                        results.append(RetrievalResult(\n",
        "                            id=rel.id,\n",
        "                            content=rel.to_text(),\n",
        "                            score=rel.weight,\n",
        "                            source='graph',\n",
        "                            metadata={\n",
        "                                'source': rel.source,\n",
        "                                'target': rel.target,\n",
        "                                'relation_type': rel.relation_type\n",
        "                            }\n",
        "                        ))\n",
        "            \n",
        "            # Incoming relationships\n",
        "            for source, _, data in self.graph.in_edges(entity_id, data=True):\n",
        "                rel_id = data.get('id')\n",
        "                if rel_id and rel_id not in seen_rels:\n",
        "                    seen_rels.add(rel_id)\n",
        "                    rel = self.relationships.get(rel_id)\n",
        "                    if rel:\n",
        "                        results.append(RetrievalResult(\n",
        "                            id=rel.id,\n",
        "                            content=rel.to_text(),\n",
        "                            score=rel.weight,\n",
        "                            source='graph',\n",
        "                            metadata={\n",
        "                                'source': rel.source,\n",
        "                                'target': rel.target,\n",
        "                                'relation_type': rel.relation_type\n",
        "                            }\n",
        "                        ))\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get graph statistics.\"\"\"\n",
        "        return {\n",
        "            'num_entities': len(self.entities),\n",
        "            'num_relationships': len(self.relationships),\n",
        "            'num_nodes': self.graph.number_of_nodes(),\n",
        "            'num_edges': self.graph.number_of_edges(),\n",
        "            'density': nx.density(self.graph) if self.graph.number_of_nodes() > 0 else 0\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"âœ… KnowledgeGraph class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”€ 6. Hybrid Retriever\n",
        "\n",
        "The Hybrid Retriever combines vector search and graph search with multiple fusion strategies:\n",
        "- **Reciprocal Rank Fusion (RRF)**: Combines rankings from multiple sources\n",
        "- **Weighted Score Fusion**: Weighted average of normalized scores\n",
        "- **Graph-Enhanced Vector Search**: Uses graph context to boost vector results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridRetriever:\n",
        "    \"\"\"\n",
        "    Hybrid retrieval system combining vector search and graph-based search.\n",
        "    \n",
        "    Fusion Strategies:\n",
        "    1. RRF (Reciprocal Rank Fusion): Rank-based fusion\n",
        "    2. Weighted: Score-based weighted combination\n",
        "    3. Graph-Enhanced: Use graph to boost/filter vector results\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vector_store: VectorStore, knowledge_graph: KnowledgeGraph):\n",
        "        self.vector_store = vector_store\n",
        "        self.knowledge_graph = knowledge_graph\n",
        "        print(\"âœ… HybridRetriever initialized\")\n",
        "    \n",
        "    def extract_entities_from_query(self, query: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract potential entity names from a query.\n",
        "        Simple approach: match against known entity names.\n",
        "        \"\"\"\n",
        "        query_lower = query.lower()\n",
        "        matched_entities = []\n",
        "        \n",
        "        for entity_name in self.knowledge_graph.entity_name_to_id.keys():\n",
        "            if entity_name in query_lower:\n",
        "                matched_entities.append(entity_name)\n",
        "        \n",
        "        return matched_entities\n",
        "    \n",
        "    def vector_search(self, query: str, collection: str = 'documents', \n",
        "                      k: int = 5) -> List[RetrievalResult]:\n",
        "        \"\"\"Perform vector similarity search.\"\"\"\n",
        "        return self.vector_store.search(collection, query, k)\n",
        "    \n",
        "    def graph_search(self, query: str, depth: int = 2, \n",
        "                     k: int = 5) -> List[RetrievalResult]:\n",
        "        \"\"\"Perform graph-based search starting from entities in the query.\"\"\"\n",
        "        entities = self.extract_entities_from_query(query)\n",
        "        \n",
        "        if not entities:\n",
        "            # Fall back to searching entities collection by vector similarity\n",
        "            entity_results = self.vector_store.search('entities', query, k=3)\n",
        "            entities = [r.metadata.get('name', '') for r in entity_results if r.metadata.get('name')]\n",
        "        \n",
        "        results = self.knowledge_graph.search_by_entity(entities, depth=depth, k=k)\n",
        "        \n",
        "        # Also add relationship context\n",
        "        entity_ids = [self.knowledge_graph.entity_name_to_id.get(e.lower()) \n",
        "                      for e in entities if e.lower() in self.knowledge_graph.entity_name_to_id]\n",
        "        entity_ids = [eid for eid in entity_ids if eid]\n",
        "        \n",
        "        if entity_ids:\n",
        "            rel_results = self.knowledge_graph.get_relationship_context(entity_ids)\n",
        "            results.extend(rel_results)\n",
        "        \n",
        "        return results[:k]\n",
        "    \n",
        "    def reciprocal_rank_fusion(self, result_lists: List[List[RetrievalResult]], \n",
        "                                k: int = 60) -> List[RetrievalResult]:\n",
        "        \"\"\"\n",
        "        Reciprocal Rank Fusion (RRF) algorithm.\n",
        "        \n",
        "        Combines multiple ranked lists using: score = sum(1 / (k + rank))\n",
        "        \"\"\"\n",
        "        scores: Dict[str, float] = defaultdict(float)\n",
        "        result_map: Dict[str, RetrievalResult] = {}\n",
        "        \n",
        "        for result_list in result_lists:\n",
        "            for rank, result in enumerate(result_list):\n",
        "                scores[result.id] += 1.0 / (k + rank + 1)\n",
        "                if result.id not in result_map:\n",
        "                    result_map[result.id] = result\n",
        "        \n",
        "        # Create fused results\n",
        "        fused_results = []\n",
        "        for result_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
        "            original = result_map[result_id]\n",
        "            fused_results.append(RetrievalResult(\n",
        "                id=result_id,\n",
        "                content=original.content,\n",
        "                score=score,\n",
        "                source='hybrid_rrf',\n",
        "                metadata={**original.metadata, 'original_source': original.source}\n",
        "            ))\n",
        "        \n",
        "        return fused_results\n",
        "    \n",
        "    def weighted_fusion(self, vector_results: List[RetrievalResult],\n",
        "                        graph_results: List[RetrievalResult],\n",
        "                        vector_weight: float = 0.6,\n",
        "                        graph_weight: float = 0.4) -> List[RetrievalResult]:\n",
        "        \"\"\"\n",
        "        Weighted score fusion with normalized scores.\n",
        "        \"\"\"\n",
        "        scores: Dict[str, float] = defaultdict(float)\n",
        "        result_map: Dict[str, RetrievalResult] = {}\n",
        "        \n",
        "        # Normalize and weight vector results\n",
        "        if vector_results:\n",
        "            max_v_score = max(r.score for r in vector_results)\n",
        "            for result in vector_results:\n",
        "                norm_score = result.score / max_v_score if max_v_score > 0 else 0\n",
        "                scores[result.id] += vector_weight * norm_score\n",
        "                result_map[result.id] = result\n",
        "        \n",
        "        # Normalize and weight graph results\n",
        "        if graph_results:\n",
        "            max_g_score = max(r.score for r in graph_results)\n",
        "            for result in graph_results:\n",
        "                norm_score = result.score / max_g_score if max_g_score > 0 else 0\n",
        "                scores[result.id] += graph_weight * norm_score\n",
        "                if result.id not in result_map:\n",
        "                    result_map[result.id] = result\n",
        "        \n",
        "        # Create fused results\n",
        "        fused_results = []\n",
        "        for result_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
        "            original = result_map[result_id]\n",
        "            fused_results.append(RetrievalResult(\n",
        "                id=result_id,\n",
        "                content=original.content,\n",
        "                score=score,\n",
        "                source='hybrid_weighted',\n",
        "                metadata={**original.metadata, 'original_source': original.source}\n",
        "            ))\n",
        "        \n",
        "        return fused_results\n",
        "    \n",
        "    def graph_enhanced_search(self, query: str, k: int = 5,\n",
        "                              boost_factor: float = 1.5) -> List[RetrievalResult]:\n",
        "        \"\"\"\n",
        "        Graph-enhanced vector search.\n",
        "        \n",
        "        Uses graph context to boost scores of vector results that are\n",
        "        connected to entities mentioned in the query.\n",
        "        \"\"\"\n",
        "        # Get vector results\n",
        "        vector_results = self.vector_search(query, k=k*2)\n",
        "        \n",
        "        # Extract entities from query\n",
        "        query_entities = self.extract_entities_from_query(query)\n",
        "        \n",
        "        if not query_entities:\n",
        "            return vector_results[:k]\n",
        "        \n",
        "        # Get entity IDs\n",
        "        query_entity_ids = set()\n",
        "        for name in query_entities:\n",
        "            eid = self.knowledge_graph.entity_name_to_id.get(name.lower())\n",
        "            if eid:\n",
        "                query_entity_ids.add(eid)\n",
        "                # Also add neighbors\n",
        "                query_entity_ids.update(\n",
        "                    self.knowledge_graph.get_neighbors(eid, depth=1)\n",
        "                )\n",
        "        \n",
        "        # Boost scores for results mentioning connected entities\n",
        "        boosted_results = []\n",
        "        for result in vector_results:\n",
        "            boost = 1.0\n",
        "            content_lower = result.content.lower()\n",
        "            \n",
        "            # Check if result mentions any connected entities\n",
        "            for entity_id in query_entity_ids:\n",
        "                entity = self.knowledge_graph.entities.get(entity_id)\n",
        "                if entity and entity.name.lower() in content_lower:\n",
        "                    boost = boost_factor\n",
        "                    break\n",
        "            \n",
        "            boosted_results.append(RetrievalResult(\n",
        "                id=result.id,\n",
        "                content=result.content,\n",
        "                score=result.score * boost,\n",
        "                source='hybrid_graph_enhanced',\n",
        "                metadata={**result.metadata, 'boost_applied': boost > 1.0}\n",
        "            ))\n",
        "        \n",
        "        boosted_results.sort(key=lambda x: x.score, reverse=True)\n",
        "        return boosted_results[:k]\n",
        "    \n",
        "    def retrieve(self, query: str, k: int = 5,\n",
        "                 method: str = 'rrf',\n",
        "                 vector_weight: float = 0.6) -> HybridResult:\n",
        "        \"\"\"\n",
        "        Perform hybrid retrieval with the specified fusion method.\n",
        "        \n",
        "        Args:\n",
        "            query: Search query\n",
        "            k: Number of results to return\n",
        "            method: 'rrf', 'weighted', or 'graph_enhanced'\n",
        "            vector_weight: Weight for vector results (weighted fusion only)\n",
        "        \"\"\"\n",
        "        # Get results from both sources\n",
        "        vector_results = self.vector_search(query, k=k*2)\n",
        "        graph_results = self.graph_search(query, k=k*2)\n",
        "        \n",
        "        # Apply fusion\n",
        "        if method == 'rrf':\n",
        "            fused = self.reciprocal_rank_fusion([vector_results, graph_results])\n",
        "        elif method == 'weighted':\n",
        "            fused = self.weighted_fusion(\n",
        "                vector_results, graph_results,\n",
        "                vector_weight=vector_weight,\n",
        "                graph_weight=1-vector_weight\n",
        "            )\n",
        "        elif method == 'graph_enhanced':\n",
        "            fused = self.graph_enhanced_search(query, k=k)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown fusion method: {method}\")\n",
        "        \n",
        "        return HybridResult(\n",
        "            query=query,\n",
        "            vector_results=vector_results[:k],\n",
        "            graph_results=graph_results[:k],\n",
        "            fused_results=fused[:k],\n",
        "            fusion_method=method\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"âœ… HybridRetriever class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š 7. Entity Extraction & Knowledge Graph Building\n",
        "\n",
        "For this demo, we define entities and relationships manually (simulating what GraphRAG would extract with an LLM). In production, you would use an LLM to extract these from your documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define entities (extracted from documents)\n",
        "ENTITIES = [\n",
        "    # People\n",
        "    Entity(\"e1\", \"Sarah Chen\", \"PERSON\", \n",
        "           \"CEO of TechCorp, co-founder, previously worked at Google and Stanford AI Lab\",\n",
        "           source_docs=[\"doc_1\", \"doc_2\", \"doc_6\"]),\n",
        "    Entity(\"e2\", \"Michael Rodriguez\", \"PERSON\",\n",
        "           \"CTO of TechCorp, co-founder, PhD in Machine Learning from MIT, oversees R&D\",\n",
        "           source_docs=[\"doc_1\", \"doc_2\", \"doc_7\"]),\n",
        "    Entity(\"e3\", \"Jennifer Park\", \"PERSON\",\n",
        "           \"CFO of TechCorp, joined 2019 from Goldman Sachs, led successful IPO\",\n",
        "           source_docs=[\"doc_2\", \"doc_5\"]),\n",
        "    Entity(\"e4\", \"David Thompson\", \"PERSON\",\n",
        "           \"Head of Sales at TechCorp, expanded customer base to Fortune 500\",\n",
        "           source_docs=[\"doc_2\", \"doc_8\"]),\n",
        "    Entity(\"e5\", \"Dr. Emily Watson\", \"PERSON\",\n",
        "           \"Head of R&D at TechCorp, published 50+ papers, breakthrough in transformer architectures\",\n",
        "           source_docs=[\"doc_4\"]),\n",
        "    \n",
        "    # Organizations\n",
        "    Entity(\"e6\", \"TechCorp\", \"ORGANIZATION\",\n",
        "           \"Leading AI technology company founded in 2015, 5000+ employees, $2.5B revenue\",\n",
        "           source_docs=[\"doc_1\", \"doc_2\", \"doc_3\", \"doc_4\", \"doc_5\", \"doc_6\", \"doc_7\", \"doc_8\"]),\n",
        "    Entity(\"e7\", \"Google\", \"ORGANIZATION\",\n",
        "           \"Tech company where Sarah Chen previously worked\",\n",
        "           source_docs=[\"doc_2\"]),\n",
        "    Entity(\"e8\", \"Stanford AI Lab\", \"ORGANIZATION\",\n",
        "           \"Research institution where Sarah Chen previously worked\",\n",
        "           source_docs=[\"doc_2\"]),\n",
        "    Entity(\"e9\", \"MIT\", \"ORGANIZATION\",\n",
        "           \"University where Michael Rodriguez got his PhD, research partner\",\n",
        "           source_docs=[\"doc_2\", \"doc_4\"]),\n",
        "    Entity(\"e10\", \"Goldman Sachs\", \"ORGANIZATION\",\n",
        "           \"Investment bank where Jennifer Park worked before joining TechCorp\",\n",
        "           source_docs=[\"doc_2\"]),\n",
        "    Entity(\"e11\", \"Stanford University\", \"ORGANIZATION\",\n",
        "           \"Research partner of TechCorp\",\n",
        "           source_docs=[\"doc_4\"]),\n",
        "    Entity(\"e12\", \"Carnegie Mellon\", \"ORGANIZATION\",\n",
        "           \"Research partner of TechCorp\",\n",
        "           source_docs=[\"doc_4\"]),\n",
        "    Entity(\"e13\", \"Sequoia Capital\", \"ORGANIZATION\",\n",
        "           \"Major investor in TechCorp\",\n",
        "           source_docs=[\"doc_5\"]),\n",
        "    Entity(\"e14\", \"Andreessen Horowitz\", \"ORGANIZATION\",\n",
        "           \"Major investor in TechCorp\",\n",
        "           source_docs=[\"doc_5\"]),\n",
        "    Entity(\"e15\", \"SoftBank Vision Fund\", \"ORGANIZATION\",\n",
        "           \"Major investor in TechCorp\",\n",
        "           source_docs=[\"doc_5\"]),\n",
        "    \n",
        "    # Products\n",
        "    Entity(\"e16\", \"AIAssist Pro\", \"PRODUCT\",\n",
        "           \"TechCorp's flagship enterprise AI assistant for customer service automation, 200+ customers\",\n",
        "           source_docs=[\"doc_3\", \"doc_7\"]),\n",
        "    Entity(\"e17\", \"DataSense Analytics\", \"PRODUCT\",\n",
        "           \"TechCorp's predictive analytics product for supply chain optimization\",\n",
        "           source_docs=[\"doc_3\"]),\n",
        "    Entity(\"e18\", \"SecureAI\", \"PRODUCT\",\n",
        "           \"TechCorp's AI-powered cybersecurity product launched in 2024\",\n",
        "           source_docs=[\"doc_3\"]),\n",
        "    \n",
        "    # Clients\n",
        "    Entity(\"e19\", \"Walmart\", \"ORGANIZATION\",\n",
        "           \"Major retail client of TechCorp using DataSense Analytics\",\n",
        "           source_docs=[\"doc_3\", \"doc_8\"]),\n",
        "    Entity(\"e20\", \"Target\", \"ORGANIZATION\",\n",
        "           \"Retail client using DataSense Analytics, reported 30% efficiency improvements\",\n",
        "           source_docs=[\"doc_3\", \"doc_8\"]),\n",
        "    Entity(\"e21\", \"JPMorgan Chase\", \"ORGANIZATION\",\n",
        "           \"Banking client using SecureAI\",\n",
        "           source_docs=[\"doc_3\", \"doc_8\"]),\n",
        "    Entity(\"e22\", \"Bank of America\", \"ORGANIZATION\",\n",
        "           \"Banking client using SecureAI\",\n",
        "           source_docs=[\"doc_3\", \"doc_8\"]),\n",
        "    Entity(\"e23\", \"Wells Fargo\", \"ORGANIZATION\",\n",
        "           \"Banking client using SecureAI\",\n",
        "           source_docs=[\"doc_3\", \"doc_8\"]),\n",
        "    Entity(\"e24\", \"FedEx\", \"ORGANIZATION\",\n",
        "           \"Logistics partner for autonomous systems pilot\",\n",
        "           source_docs=[\"doc_6\", \"doc_8\"]),\n",
        "    Entity(\"e25\", \"UPS\", \"ORGANIZATION\",\n",
        "           \"Logistics partner for autonomous systems pilot\",\n",
        "           source_docs=[\"doc_6\", \"doc_8\"]),\n",
        "    Entity(\"e26\", \"Mayo Clinic\", \"ORGANIZATION\",\n",
        "           \"Healthcare partner for AI expansion\",\n",
        "           source_docs=[\"doc_6\"]),\n",
        "    Entity(\"e27\", \"Cleveland Clinic\", \"ORGANIZATION\",\n",
        "           \"Healthcare partner for AI expansion\",\n",
        "           source_docs=[\"doc_6\"]),\n",
        "    Entity(\"e28\", \"Amazon\", \"ORGANIZATION\",\n",
        "           \"Strategic partner and client\",\n",
        "           source_docs=[\"doc_8\"]),\n",
        "    Entity(\"e29\", \"Microsoft\", \"ORGANIZATION\",\n",
        "           \"Strategic partner\",\n",
        "           source_docs=[\"doc_8\"]),\n",
        "    \n",
        "    # Locations\n",
        "    Entity(\"e30\", \"San Francisco\", \"LOCATION\",\n",
        "           \"TechCorp headquarters location\",\n",
        "           source_docs=[\"doc_1\"]),\n",
        "    Entity(\"e31\", \"London\", \"LOCATION\",\n",
        "           \"Planned R&D center location\",\n",
        "           source_docs=[\"doc_6\"]),\n",
        "    Entity(\"e32\", \"Singapore\", \"LOCATION\",\n",
        "           \"Planned R&D center location\",\n",
        "           source_docs=[\"doc_6\"]),\n",
        "    Entity(\"e33\", \"Tel Aviv\", \"LOCATION\",\n",
        "           \"Planned R&D center location\",\n",
        "           source_docs=[\"doc_6\"]),\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“Š Defined {len(ENTITIES)} entities\")\n",
        "for entity_type in set(e.type for e in ENTITIES):\n",
        "    count = sum(1 for e in ENTITIES if e.type == entity_type)\n",
        "    print(f\"   - {entity_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define relationships\n",
        "RELATIONSHIPS = [\n",
        "    # Leadership relationships\n",
        "    Relationship(\"r1\", \"e1\", \"e6\", \"CEO_OF\", \"Sarah Chen is the CEO of TechCorp\"),\n",
        "    Relationship(\"r2\", \"e2\", \"e6\", \"CTO_OF\", \"Michael Rodriguez is the CTO of TechCorp\"),\n",
        "    Relationship(\"r3\", \"e3\", \"e6\", \"CFO_OF\", \"Jennifer Park is the CFO of TechCorp\"),\n",
        "    Relationship(\"r4\", \"e4\", \"e6\", \"LEADS_SALES\", \"David Thompson leads Sales at TechCorp\"),\n",
        "    Relationship(\"r5\", \"e5\", \"e6\", \"LEADS_RD\", \"Dr. Emily Watson leads R&D at TechCorp\"),\n",
        "    \n",
        "    # Founding relationships\n",
        "    Relationship(\"r6\", \"e1\", \"e6\", \"FOUNDED\", \"Sarah Chen co-founded TechCorp in 2015\"),\n",
        "    Relationship(\"r7\", \"e2\", \"e6\", \"FOUNDED\", \"Michael Rodriguez co-founded TechCorp in 2015\"),\n",
        "    \n",
        "    # Previous employment\n",
        "    Relationship(\"r8\", \"e1\", \"e7\", \"WORKED_AT\", \"Sarah Chen previously worked at Google\"),\n",
        "    Relationship(\"r9\", \"e1\", \"e8\", \"WORKED_AT\", \"Sarah Chen previously worked at Stanford AI Lab\"),\n",
        "    Relationship(\"r10\", \"e2\", \"e9\", \"EDUCATED_AT\", \"Michael Rodriguez got PhD from MIT\"),\n",
        "    Relationship(\"r11\", \"e3\", \"e10\", \"WORKED_AT\", \"Jennifer Park worked at Goldman Sachs before TechCorp\"),\n",
        "    \n",
        "    # Product relationships\n",
        "    Relationship(\"r12\", \"e6\", \"e16\", \"DEVELOPS\", \"TechCorp develops AIAssist Pro\"),\n",
        "    Relationship(\"r13\", \"e6\", \"e17\", \"DEVELOPS\", \"TechCorp develops DataSense Analytics\"),\n",
        "    Relationship(\"r14\", \"e6\", \"e18\", \"DEVELOPS\", \"TechCorp develops SecureAI\"),\n",
        "    Relationship(\"r15\", \"e2\", \"e16\", \"ARCHITECTED\", \"Michael Rodriguez architected AIAssist Pro platform\"),\n",
        "    \n",
        "    # Client relationships\n",
        "    Relationship(\"r16\", \"e19\", \"e17\", \"USES\", \"Walmart uses DataSense Analytics\"),\n",
        "    Relationship(\"r17\", \"e20\", \"e17\", \"USES\", \"Target uses DataSense Analytics\"),\n",
        "    Relationship(\"r18\", \"e21\", \"e18\", \"USES\", \"JPMorgan Chase uses SecureAI\"),\n",
        "    Relationship(\"r19\", \"e22\", \"e18\", \"USES\", \"Bank of America uses SecureAI\"),\n",
        "    Relationship(\"r20\", \"e23\", \"e18\", \"USES\", \"Wells Fargo uses SecureAI\"),\n",
        "    \n",
        "    # Partnership relationships\n",
        "    Relationship(\"r21\", \"e6\", \"e11\", \"PARTNERS_WITH\", \"TechCorp collaborates with Stanford University\"),\n",
        "    Relationship(\"r22\", \"e6\", \"e9\", \"PARTNERS_WITH\", \"TechCorp collaborates with MIT\"),\n",
        "    Relationship(\"r23\", \"e6\", \"e12\", \"PARTNERS_WITH\", \"TechCorp collaborates with Carnegie Mellon\"),\n",
        "    Relationship(\"r24\", \"e6\", \"e24\", \"PARTNERS_WITH\", \"TechCorp partners with FedEx for logistics AI\"),\n",
        "    Relationship(\"r25\", \"e6\", \"e25\", \"PARTNERS_WITH\", \"TechCorp partners with UPS for logistics AI\"),\n",
        "    Relationship(\"r26\", \"e6\", \"e26\", \"PARTNERS_WITH\", \"TechCorp partners with Mayo Clinic for healthcare AI\"),\n",
        "    Relationship(\"r27\", \"e6\", \"e27\", \"PARTNERS_WITH\", \"TechCorp partners with Cleveland Clinic\"),\n",
        "    Relationship(\"r28\", \"e6\", \"e28\", \"PARTNERS_WITH\", \"TechCorp has strategic partnership with Amazon\"),\n",
        "    Relationship(\"r29\", \"e6\", \"e29\", \"PARTNERS_WITH\", \"TechCorp has strategic partnership with Microsoft\"),\n",
        "    \n",
        "    # Investment relationships\n",
        "    Relationship(\"r30\", \"e13\", \"e6\", \"INVESTED_IN\", \"Sequoia Capital invested in TechCorp\"),\n",
        "    Relationship(\"r31\", \"e14\", \"e6\", \"INVESTED_IN\", \"Andreessen Horowitz invested in TechCorp\"),\n",
        "    Relationship(\"r32\", \"e15\", \"e6\", \"INVESTED_IN\", \"SoftBank Vision Fund invested in TechCorp\"),\n",
        "    \n",
        "    # Location relationships\n",
        "    Relationship(\"r33\", \"e6\", \"e30\", \"HEADQUARTERED_IN\", \"TechCorp is headquartered in San Francisco\"),\n",
        "    Relationship(\"r34\", \"e6\", \"e31\", \"EXPANDING_TO\", \"TechCorp plans R&D center in London\"),\n",
        "    Relationship(\"r35\", \"e6\", \"e32\", \"EXPANDING_TO\", \"TechCorp plans R&D center in Singapore\"),\n",
        "    Relationship(\"r36\", \"e6\", \"e33\", \"EXPANDING_TO\", \"TechCorp plans R&D center in Tel Aviv\"),\n",
        "]\n",
        "\n",
        "print(f\"ðŸ”— Defined {len(RELATIONSHIPS)} relationships\")\n",
        "for rel_type in set(r.relation_type for r in RELATIONSHIPS):\n",
        "    count = sum(1 for r in RELATIONSHIPS if r.relation_type == rel_type)\n",
        "    print(f\"   - {rel_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ—ï¸ 8. Building the Hybrid Retrieval System\n",
        "\n",
        "Now let's initialize all components and populate them with our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Vector Store\n",
        "print(\"=\" * 60)\n",
        "print(\"Initializing Vector Store...\")\n",
        "print(\"=\" * 60)\n",
        "vector_store = VectorStore(embedding_model=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Add documents to vector store\n",
        "print(\"\\nðŸ“„ Adding documents to vector store...\")\n",
        "vector_store.add_documents('documents', [\n",
        "    {'id': doc['id'], 'content': doc['content'], 'metadata': {'title': doc['title']}}\n",
        "    for doc in DOCUMENTS\n",
        "])\n",
        "\n",
        "# Add entities to vector store\n",
        "print(\"\\nðŸ·ï¸ Adding entities to vector store...\")\n",
        "vector_store.add_entities(ENTITIES)\n",
        "\n",
        "# Add relationships to vector store\n",
        "print(\"\\nðŸ”— Adding relationships to vector store...\")\n",
        "vector_store.add_relationships(RELATIONSHIPS)\n",
        "\n",
        "print(\"\\nâœ… Vector Store populated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Knowledge Graph\n",
        "print(\"=\" * 60)\n",
        "print(\"Initializing Knowledge Graph...\")\n",
        "print(\"=\" * 60)\n",
        "knowledge_graph = KnowledgeGraph()\n",
        "\n",
        "# Add entities\n",
        "print(\"\\nðŸ·ï¸ Adding entities to graph...\")\n",
        "for entity in ENTITIES:\n",
        "    knowledge_graph.add_entity(entity)\n",
        "\n",
        "# Add relationships\n",
        "print(\"ðŸ”— Adding relationships to graph...\")\n",
        "for relationship in RELATIONSHIPS:\n",
        "    knowledge_graph.add_relationship(relationship)\n",
        "\n",
        "# Print graph statistics\n",
        "stats = knowledge_graph.get_stats()\n",
        "print(f\"\\nðŸ“Š Graph Statistics:\")\n",
        "print(f\"   Entities: {stats['num_entities']}\")\n",
        "print(f\"   Relationships: {stats['num_relationships']}\")\n",
        "print(f\"   Graph Density: {stats['density']:.4f}\")\n",
        "\n",
        "print(\"\\nâœ… Knowledge Graph populated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Hybrid Retriever\n",
        "print(\"=\" * 60)\n",
        "print(\"Initializing Hybrid Retriever...\")\n",
        "print(\"=\" * 60)\n",
        "hybrid_retriever = HybridRetriever(vector_store, knowledge_graph)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Hybrid Retrieval System Ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” 9. Demonstration: Hybrid Retrieval in Action\n",
        "\n",
        "Let's test our hybrid retrieval system with different queries and fusion strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_results(result: HybridResult, show_all: bool = False):\n",
        "    \"\"\"Pretty print hybrid retrieval results.\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"ðŸ” QUERY: {result.query}\")\n",
        "    print(f\"ðŸ“Š Fusion Method: {result.fusion_method.upper()}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    if show_all:\n",
        "        print(\"\\nðŸ“Œ VECTOR SEARCH RESULTS:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, r in enumerate(result.vector_results[:5], 1):\n",
        "            print(f\"  {i}. [{r.score:.3f}] {r.content[:80]}...\")\n",
        "        \n",
        "        print(\"\\nðŸ“Œ GRAPH SEARCH RESULTS:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, r in enumerate(result.graph_results[:5], 1):\n",
        "            print(f\"  {i}. [{r.score:.3f}] {r.content[:80]}...\")\n",
        "    \n",
        "    print(\"\\nðŸŽ¯ FUSED RESULTS (Top 5):\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, r in enumerate(result.fused_results[:5], 1):\n",
        "        orig_source = r.metadata.get('original_source', 'unknown')\n",
        "        print(f\"  {i}. [{r.score:.3f}] (from: {orig_source})\")\n",
        "        print(f\"      {r.content[:100]}...\")\n",
        "        print()\n",
        "\n",
        "\n",
        "# Test Query 1: Entity-focused query\n",
        "query1 = \"Who is Sarah Chen and what is her role?\"\n",
        "print(\"\\n\" + \"ðŸ”¸\" * 35)\n",
        "print(\"TEST 1: Entity-focused query with RRF fusion\")\n",
        "print(\"ðŸ”¸\" * 35)\n",
        "result1 = hybrid_retriever.retrieve(query1, k=5, method='rrf')\n",
        "display_results(result1, show_all=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Query 2: Product/relationship query with Weighted fusion\n",
        "query2 = \"What products does TechCorp offer and who are the clients?\"\n",
        "print(\"\\n\" + \"ðŸ”¸\" * 35)\n",
        "print(\"TEST 2: Product query with WEIGHTED fusion (70% vector, 30% graph)\")\n",
        "print(\"ðŸ”¸\" * 35)\n",
        "result2 = hybrid_retriever.retrieve(query2, k=5, method='weighted', vector_weight=0.7)\n",
        "display_results(result2, show_all=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Query 3: Graph-enhanced search (best for relationship queries)\n",
        "query3 = \"What are TechCorp's partnerships with universities?\"\n",
        "print(\"\\n\" + \"ðŸ”¸\" * 35)\n",
        "print(\"TEST 3: Relationship query with GRAPH-ENHANCED search\")\n",
        "print(\"ðŸ”¸\" * 35)\n",
        "result3 = hybrid_retriever.retrieve(query3, k=5, method='graph_enhanced')\n",
        "display_results(result3, show_all=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Query 4: Compare all fusion methods on the same query\n",
        "query4 = \"Who founded TechCorp and what are the company's main investors?\"\n",
        "print(\"\\n\" + \"ðŸ”¹\" * 35)\n",
        "print(\"TEST 4: Comparing ALL fusion methods on the same query\")\n",
        "print(\"ðŸ”¹\" * 35)\n",
        "\n",
        "methods = ['rrf', 'weighted', 'graph_enhanced']\n",
        "for method in methods:\n",
        "    result = hybrid_retriever.retrieve(query4, k=3, method=method)\n",
        "    print(f\"\\nðŸ“Š {method.upper()} Results:\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, r in enumerate(result.fused_results[:3], 1):\n",
        "        print(f\"  {i}. [{r.score:.3f}] {r.content[:70]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¨ 10. Knowledge Graph Visualization\n",
        "\n",
        "Let's visualize the knowledge graph to understand the entity relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_knowledge_graph(kg: KnowledgeGraph, figsize: Tuple[int, int] = (16, 12)):\n",
        "    \"\"\"Visualize the knowledge graph with colored nodes by entity type.\"\"\"\n",
        "    G = kg.graph\n",
        "    \n",
        "    # Define colors for entity types\n",
        "    type_colors = {\n",
        "        'PERSON': '#FF6B6B',       # Red\n",
        "        'ORGANIZATION': '#4ECDC4', # Teal\n",
        "        'PRODUCT': '#45B7D1',      # Blue\n",
        "        'LOCATION': '#96CEB4',     # Green\n",
        "    }\n",
        "    \n",
        "    # Get node colors\n",
        "    node_colors = []\n",
        "    for node in G.nodes():\n",
        "        entity = kg.entities.get(node)\n",
        "        if entity:\n",
        "            node_colors.append(type_colors.get(entity.type, '#CCCCCC'))\n",
        "        else:\n",
        "            node_colors.append('#CCCCCC')\n",
        "    \n",
        "    # Get node labels (use names instead of IDs)\n",
        "    labels = {}\n",
        "    for node in G.nodes():\n",
        "        entity = kg.entities.get(node)\n",
        "        if entity:\n",
        "            # Truncate long names\n",
        "            name = entity.name\n",
        "            if len(name) > 15:\n",
        "                name = name[:12] + \"...\"\n",
        "            labels[node] = name\n",
        "        else:\n",
        "            labels[node] = node\n",
        "    \n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "    \n",
        "    # Layout\n",
        "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
        "    \n",
        "    # Draw edges with arrows\n",
        "    nx.draw_networkx_edges(G, pos, ax=ax, \n",
        "                           edge_color='#CCCCCC', \n",
        "                           arrows=True,\n",
        "                           arrowsize=15,\n",
        "                           connectionstyle=\"arc3,rad=0.1\",\n",
        "                           alpha=0.6)\n",
        "    \n",
        "    # Draw nodes\n",
        "    nx.draw_networkx_nodes(G, pos, ax=ax,\n",
        "                           node_color=node_colors,\n",
        "                           node_size=800,\n",
        "                           alpha=0.9)\n",
        "    \n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, labels, ax=ax,\n",
        "                           font_size=8,\n",
        "                           font_weight='bold')\n",
        "    \n",
        "    # Add legend\n",
        "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                  markerfacecolor=color, markersize=12, label=etype)\n",
        "                      for etype, color in type_colors.items()]\n",
        "    ax.legend(handles=legend_elements, loc='upper left', title='Entity Types')\n",
        "    \n",
        "    ax.set_title('TechCorp Knowledge Graph', fontsize=16, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n",
        "# Visualize the knowledge graph\n",
        "fig = visualize_knowledge_graph(knowledge_graph)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a focused subgraph around TechCorp\n",
        "def visualize_subgraph(kg: KnowledgeGraph, center_entity: str, depth: int = 1):\n",
        "    \"\"\"Visualize a subgraph centered on a specific entity.\"\"\"\n",
        "    entity = kg.get_entity_by_name(center_entity)\n",
        "    if not entity:\n",
        "        print(f\"Entity '{center_entity}' not found\")\n",
        "        return\n",
        "    \n",
        "    # Get subgraph\n",
        "    subgraph = kg.get_subgraph([entity.id], include_neighbors=True, neighbor_depth=depth)\n",
        "    \n",
        "    # Define colors\n",
        "    type_colors = {\n",
        "        'PERSON': '#FF6B6B',\n",
        "        'ORGANIZATION': '#4ECDC4',\n",
        "        'PRODUCT': '#45B7D1',\n",
        "        'LOCATION': '#96CEB4',\n",
        "    }\n",
        "    \n",
        "    # Setup\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Node colors and sizes\n",
        "    node_colors = []\n",
        "    node_sizes = []\n",
        "    labels = {}\n",
        "    \n",
        "    for node in subgraph.nodes():\n",
        "        e = kg.entities.get(node)\n",
        "        if e:\n",
        "            node_colors.append(type_colors.get(e.type, '#CCCCCC'))\n",
        "            # Make center entity larger\n",
        "            node_sizes.append(1500 if node == entity.id else 800)\n",
        "            labels[node] = e.name if len(e.name) <= 15 else e.name[:12] + \"...\"\n",
        "    \n",
        "    pos = nx.spring_layout(subgraph, k=2, seed=42)\n",
        "    \n",
        "    # Draw\n",
        "    nx.draw_networkx_edges(subgraph, pos, ax=ax, edge_color='#888888', \n",
        "                           arrows=True, arrowsize=20, alpha=0.7)\n",
        "    nx.draw_networkx_nodes(subgraph, pos, ax=ax, node_color=node_colors,\n",
        "                           node_size=node_sizes, alpha=0.9)\n",
        "    nx.draw_networkx_labels(subgraph, pos, labels, ax=ax, font_size=9, font_weight='bold')\n",
        "    \n",
        "    # Edge labels\n",
        "    edge_labels = {}\n",
        "    for u, v, data in subgraph.edges(data=True):\n",
        "        rel_type = data.get('relation_type', '')\n",
        "        edge_labels[(u, v)] = rel_type\n",
        "    nx.draw_networkx_edge_labels(subgraph, pos, edge_labels, ax=ax, font_size=7)\n",
        "    \n",
        "    ax.set_title(f\"Subgraph: {center_entity} (depth={depth})\", fontsize=14, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Show TechCorp's immediate relationships\n",
        "print(\"ðŸ“Š TechCorp's 1-hop neighborhood:\")\n",
        "visualize_subgraph(knowledge_graph, \"TechCorp\", depth=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š 11. Performance Analysis\n",
        "\n",
        "Let's analyze how the different retrieval methods perform.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def benchmark_retrieval(retriever: HybridRetriever, queries: List[str], methods: List[str], runs: int = 5):\n",
        "    \"\"\"Benchmark retrieval methods.\"\"\"\n",
        "    results = {method: {'times': [], 'avg_score': []} for method in methods}\n",
        "    \n",
        "    for query in queries:\n",
        "        for method in methods:\n",
        "            times = []\n",
        "            scores = []\n",
        "            for _ in range(runs):\n",
        "                start = time.time()\n",
        "                result = retriever.retrieve(query, k=5, method=method)\n",
        "                elapsed = time.time() - start\n",
        "                times.append(elapsed)\n",
        "                if result.fused_results:\n",
        "                    scores.append(np.mean([r.score for r in result.fused_results]))\n",
        "            \n",
        "            results[method]['times'].extend(times)\n",
        "            results[method]['avg_score'].extend(scores)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Benchmark queries\n",
        "benchmark_queries = [\n",
        "    \"Who is the CEO of TechCorp?\",\n",
        "    \"What products does TechCorp develop?\",\n",
        "    \"Which banks use TechCorp's cybersecurity product?\",\n",
        "    \"Who are TechCorp's investors?\",\n",
        "    \"What universities does TechCorp partner with?\"\n",
        "]\n",
        "\n",
        "print(\"â±ï¸ Running benchmark (this may take a few seconds)...\")\n",
        "benchmark_results = benchmark_retrieval(hybrid_retriever, benchmark_queries, \n",
        "                                        ['rrf', 'weighted', 'graph_enhanced'], runs=3)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“Š BENCHMARK RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n{'Method':<20} {'Avg Time (ms)':<15} {'Avg Score':<15}\")\n",
        "print(\"-\" * 50)\n",
        "for method, data in benchmark_results.items():\n",
        "    avg_time = np.mean(data['times']) * 1000  # Convert to ms\n",
        "    avg_score = np.mean(data['avg_score']) if data['avg_score'] else 0\n",
        "    print(f\"{method:<20} {avg_time:<15.2f} {avg_score:<15.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š 12. Summary & Best Practices\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Vector Search (ChromaDB)**\n",
        "   - Uses sentence embeddings for semantic similarity\n",
        "   - Best for: Finding semantically similar content, paraphrase matching\n",
        "   - Limitation: Doesn't capture explicit relationships\n",
        "\n",
        "2. **Graph Search (NetworkX)**\n",
        "   - Uses graph traversal for relationship-based retrieval\n",
        "   - Best for: Multi-hop reasoning, relationship queries\n",
        "   - Limitation: Requires entity extraction, less semantic flexibility\n",
        "\n",
        "3. **Hybrid Retrieval**\n",
        "   - Combines both approaches for comprehensive retrieval\n",
        "   - Multiple fusion strategies for different use cases\n",
        "\n",
        "### Fusion Strategy Guide\n",
        "\n",
        "| Strategy | Best For | Trade-off |\n",
        "|----------|----------|-----------|\n",
        "| **RRF** | General queries, balanced results | Equal weight to all sources |\n",
        "| **Weighted** | Tuning semantic vs. structural preference | Requires weight tuning |\n",
        "| **Graph-Enhanced** | Relationship-heavy queries | Depends on entity extraction quality |\n",
        "\n",
        "### Architecture Diagram\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    HYBRID RETRIEVAL SYSTEM                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                             â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
        "â”‚  â”‚   ChromaDB   â”‚        â”‚   NetworkX   â”‚                   â”‚\n",
        "â”‚  â”‚ Vector Store â”‚        â”‚ Knowledge    â”‚                   â”‚\n",
        "â”‚  â”‚              â”‚        â”‚ Graph        â”‚                   â”‚\n",
        "â”‚  â”‚ â€¢ Documents  â”‚        â”‚              â”‚                   â”‚\n",
        "â”‚  â”‚ â€¢ Entities   â”‚        â”‚ â€¢ Entities   â”‚                   â”‚\n",
        "â”‚  â”‚ â€¢ Relations  â”‚        â”‚ â€¢ Relations  â”‚                   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
        "â”‚         â”‚                       â”‚                           â”‚\n",
        "â”‚         â”‚ Semantic Search       â”‚ Graph Traversal           â”‚\n",
        "â”‚         â”‚                       â”‚                           â”‚\n",
        "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚\n",
        "â”‚                     â”‚                                       â”‚\n",
        "â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
        "â”‚              â”‚   FUSION    â”‚                                â”‚\n",
        "â”‚              â”‚  â€¢ RRF      â”‚                                â”‚\n",
        "â”‚              â”‚  â€¢ Weighted â”‚                                â”‚\n",
        "â”‚              â”‚  â€¢ Enhanced â”‚                                â”‚\n",
        "â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
        "â”‚                     â”‚                                       â”‚\n",
        "â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
        "â”‚              â”‚   RESULTS   â”‚                                â”‚\n",
        "â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
        "â”‚                                                             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Production Considerations\n",
        "\n",
        "1. **Entity Extraction**: Use LLMs (like GPT-4) for automatic entity extraction\n",
        "2. **Scaling**: Replace in-memory stores with persistent solutions (Chroma persistent, Neo4j)\n",
        "3. **Caching**: Cache embeddings and frequently accessed graph paths\n",
        "4. **Monitoring**: Track retrieval quality metrics (MRR, recall@k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸŽ‰ Notebook Complete!\")\n",
        "print(\"\\nðŸ“š Key Takeaways:\")\n",
        "print(\"   1. Vector search excels at semantic similarity (finding related content)\")\n",
        "print(\"   2. Graph search excels at relationship traversal (finding connected entities)\")  \n",
        "print(\"   3. Hybrid retrieval combines both for comprehensive results\")\n",
        "print(\"   4. Different fusion strategies suit different query types\")\n",
        "print(\"   5. RRF is a good default; Weighted allows fine-tuning; Graph-Enhanced boosts relationships\")\n",
        "print(\"\\nðŸ”— Resources:\")\n",
        "print(\"   - ChromaDB: https://docs.trychroma.com/\")\n",
        "print(\"   - NetworkX: https://networkx.org/\")\n",
        "print(\"   - Sentence Transformers: https://www.sbert.net/\")\n",
        "print(\"   - Microsoft GraphRAG: https://github.com/microsoft/graphrag\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
